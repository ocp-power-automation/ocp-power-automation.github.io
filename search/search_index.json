{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deploying OpenShift on IBM Power Systems This Github organization is home to Infrastructure as Code (IaC) patterns to help deploy OpenShift on IBM Power Systems. Terraform is used to create the infrastructure and ansible playbooks are used to deploy and customize OpenShift. The following diagram shows the high level overview of the IaC pattern:","title":"Home"},{"location":"#deploying-openshift-on-ibm-power-systems","text":"This Github organization is home to Infrastructure as Code (IaC) patterns to help deploy OpenShift on IBM Power Systems. Terraform is used to create the infrastructure and ansible playbooks are used to deploy and customize OpenShift. The following diagram shows the high level overview of the IaC pattern:","title":"Deploying OpenShift on IBM Power Systems"},{"location":"ocp-powervm-faq/docs/powervs-faq/","text":"PowerVS Frequently Asked Questions When Building out a Cluster VM Console shows \"grub>\" Not the end of the world. Just type normal and hit ENTER. VM Console shows \"grub rescue>\" GRUB 2 was unable to find the grub folder or its contents are missing/corrupted. The grub folder contains the GRUB 2 menu, modules and stored environmental data. This should not happen very often. Simply reboot the VM from https://cloud.ibm.com . RHCOS Ignition stalls with \"version mismatch\" The version of RHCOS you are booting up with on cluster nodes cannot handle what the OCP build/version is serving as config for cluster nodes. Make sure they match at least the major and minor versions (eg. 4.5) in your var.tfvars file. Error about auth key/token Error: Error occured while fetching the auth key for power iaas: \"Post https://iam.cloud.ibm.com/identity/token: dial tcp: lookup iam.cloud.ibm.com on 192.168.2.1:53: read udp 192.168.2.25:49597->192.168.2.1:53: i/o timeout\" You may occasionally get this error upon terraform apply runs. One thing you can try is to regenerate IBM Cloud API key and rerun with it. See: https://cloud.ibm.com/docs/account?topic=account-userapikey Image Registry not coming up Toward the end of deploying a cluster or even after your terraform apply appears to complete happily, you might find your image-registry operator not AVAILABLE. Digging in, if you find the registry-pvc PersistentVolumeClaim stuck in Pending state, while nfs-client-provisioner pod appears to be Running fine, it may be the case where the backing NFS share is read-only, and thus PVCs can't be fulfilled. Check the directory ownership and permission of /export on your bastion node. It has to be owned by \"nobody\" and world writeable. Use below commands as necessary; # chown nobody:nobody /export # chmod 777 /export # exportfs -r Note: allow a few minutes to sort all things out after these commands. Problems on a Running Cluster Need to add more CPU/Memory to Node Go to https://cloud.ibm.com , then to your PowerVS service instance, find your VM that backs the cluster Node (eg. myocp-worker-0), click to open up the details page, then \"Edit details,\" update the CPU/Memory with new values, and Save. You should get \"Edit successful\" message, and your OCP cluster should recognize the change and start showing in Compute section of Web Console. If something fails along the way, try \"OS Shutdown\" action, edit CPU/Memory as necessary while in \"Shutoff\" state, and restart the VM. Securing a Cluster TBA","title":"FAQ for OpenShift on IBM Cloud Power Virtual Servers"},{"location":"ocp-powervm-faq/docs/powervs-faq/#powervs-frequently-asked-questions","text":"","title":"PowerVS Frequently Asked Questions"},{"location":"ocp-powervm-faq/docs/powervs-faq/#when-building-out-a-cluster","text":"","title":"When Building out a Cluster"},{"location":"ocp-powervm-faq/docs/powervs-faq/#vm-console-shows-grub","text":"Not the end of the world. Just type normal and hit ENTER.","title":"VM Console shows \"grub&gt;\""},{"location":"ocp-powervm-faq/docs/powervs-faq/#vm-console-shows-grub-rescue","text":"GRUB 2 was unable to find the grub folder or its contents are missing/corrupted. The grub folder contains the GRUB 2 menu, modules and stored environmental data. This should not happen very often. Simply reboot the VM from https://cloud.ibm.com .","title":"VM Console shows \"grub rescue&gt;\""},{"location":"ocp-powervm-faq/docs/powervs-faq/#rhcos-ignition-stalls-with-version-mismatch","text":"The version of RHCOS you are booting up with on cluster nodes cannot handle what the OCP build/version is serving as config for cluster nodes. Make sure they match at least the major and minor versions (eg. 4.5) in your var.tfvars file.","title":"RHCOS Ignition stalls with \"version mismatch\""},{"location":"ocp-powervm-faq/docs/powervs-faq/#error-about-auth-keytoken","text":"Error: Error occured while fetching the auth key for power iaas: \"Post https://iam.cloud.ibm.com/identity/token: dial tcp: lookup iam.cloud.ibm.com on 192.168.2.1:53: read udp 192.168.2.25:49597->192.168.2.1:53: i/o timeout\" You may occasionally get this error upon terraform apply runs. One thing you can try is to regenerate IBM Cloud API key and rerun with it. See: https://cloud.ibm.com/docs/account?topic=account-userapikey","title":"Error about auth key/token"},{"location":"ocp-powervm-faq/docs/powervs-faq/#image-registry-not-coming-up","text":"Toward the end of deploying a cluster or even after your terraform apply appears to complete happily, you might find your image-registry operator not AVAILABLE. Digging in, if you find the registry-pvc PersistentVolumeClaim stuck in Pending state, while nfs-client-provisioner pod appears to be Running fine, it may be the case where the backing NFS share is read-only, and thus PVCs can't be fulfilled. Check the directory ownership and permission of /export on your bastion node. It has to be owned by \"nobody\" and world writeable. Use below commands as necessary; # chown nobody:nobody /export # chmod 777 /export # exportfs -r Note: allow a few minutes to sort all things out after these commands.","title":"Image Registry not coming up"},{"location":"ocp-powervm-faq/docs/powervs-faq/#problems-on-a-running-cluster","text":"","title":"Problems on a Running Cluster"},{"location":"ocp-powervm-faq/docs/powervs-faq/#need-to-add-more-cpumemory-to-node","text":"Go to https://cloud.ibm.com , then to your PowerVS service instance, find your VM that backs the cluster Node (eg. myocp-worker-0), click to open up the details page, then \"Edit details,\" update the CPU/Memory with new values, and Save. You should get \"Edit successful\" message, and your OCP cluster should recognize the change and start showing in Compute section of Web Console. If something fails along the way, try \"OS Shutdown\" action, edit CPU/Memory as necessary while in \"Shutoff\" state, and restart the VM.","title":"Need to add more CPU/Memory to Node"},{"location":"ocp-powervm-faq/docs/powervs-faq/#securing-a-cluster","text":"","title":"Securing a Cluster"},{"location":"ocp-powervm-faq/docs/powervs-faq/#tba","text":"","title":"TBA"},{"location":"ocp4-upi-kvm/","text":"Table of Contents Table of Contents Introduction Automation Host Prerequisites Libvirt Prerequisites OCP Install Contributing Introduction The ocp4-upi-kvm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using libvirt. This project leverages the following ansible playbook to setup a helper node (bastion) for OCP deployment. Note For bugs/enhancement requests etc. please open a GitHub issue Warning The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches -{ release-4.5 , release-4.6 ...} and follow the docs in the specific release branches. Automation Host Prerequisites The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Linux ( preferred ) - Mac OSX (Darwin) Follow the guide to complete the prerequisites. Libvirt Prerequisites Follow the guide to complete the Libvirt prerequisites. OCP Install Follow the quickstart guide for OCP installation on KVM using libvirt. Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"Deploying OpenShift on KVM"},{"location":"ocp4-upi-kvm/#table-of-contents","text":"Table of Contents Introduction Automation Host Prerequisites Libvirt Prerequisites OCP Install Contributing","title":"Table of Contents"},{"location":"ocp4-upi-kvm/#introduction","text":"The ocp4-upi-kvm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using libvirt. This project leverages the following ansible playbook to setup a helper node (bastion) for OCP deployment. Note For bugs/enhancement requests etc. please open a GitHub issue Warning The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches -{ release-4.5 , release-4.6 ...} and follow the docs in the specific release branches.","title":"Introduction"},{"location":"ocp4-upi-kvm/#automation-host-prerequisites","text":"The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Linux ( preferred ) - Mac OSX (Darwin) Follow the guide to complete the prerequisites.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-kvm/#libvirt-prerequisites","text":"Follow the guide to complete the Libvirt prerequisites.","title":"Libvirt Prerequisites"},{"location":"ocp4-upi-kvm/#ocp-install","text":"Follow the quickstart guide for OCP installation on KVM using libvirt.","title":"OCP Install"},{"location":"ocp4-upi-kvm/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Attribution"},{"location":"ocp4-upi-kvm/CONTRIBUTING/","text":"Contributing This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project. Issues If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed. Pull Request Process To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s . Spec Formatting Conventions Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Contributing"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#contributing","text":"This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#issues","text":"If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed.","title":"Issues"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#pull-request-process","text":"To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Pull Request Process"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#spec-formatting-conventions","text":"Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Spec Formatting Conventions"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/","text":"Automation Host Prerequisites Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Terraform Providers Git RHCOS and RHEL 8.X Images for OpenShift Download the RHEL Qcow2 image Customize the RHEL Qcow2 image Download the RHCOS Qcow2 image Configure Your Firewall If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443 Automation Host Setup Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux. It's preferable to run this automation code on Linux host since Linux is required for customizing the RHEL image. Terraform Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. Terraform binary on IBM Power ( ppc64le ) is not available for download from Hashicorp website. You can download it from the following link-1 or link-2 or you will need to compile it from source by running: TAG_VERSION=v0.13.5 git clone https://github.com/hashicorp/terraform --branch $TAG $GOPATH/src/github.com/hashicorp/terraform cd $GOPATH/src/github.com/hashicorp/terraform TF_DEV=1 scripts/build.sh Validate using: $GOPATH/bin/terraform version Terraform Providers Please follow the guide to build and install the required Terraform providers. Git Git : Please refer to the link for instructions on installing Git. RHCOS and RHEL 8.X Images for OpenShift You'll need to have the RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image available on the automation host or via an HTTP(S) server. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes. Download the RHEL Qcow2 image Login to the RedHat portal and click on the Downloads tab. Click on the product Red Hat Enterprise Linux 8 from the list. In the \"Product Variant\" drop-down select \"Red Hat Enterprise Linux for Power, little endian\". In the \"Version\" drop-down select the version of RHEL you want to download. For example, will use the latest as 8.2. In the \"Product Software\" tab, click on \"Download Now\" button adjacent to \"Red Hat Enterprise Linux 8.2 Update KVM Guest Image\". Save the Qcow2 image. If your automation host is not Linux, then you will need to copy the Qcow2 image to a Linux host for the next step. Customize the RHEL Qcow2 image Customize the Qcow2 image to set a root password and disable the cloud-init service. You will need the 'libguestfs-tools' package installed on the Linux machine to run the below command (Not available on Mac/Windows). # virt-customize -a <qcow2 image file name> --root-password password:<password> --uninstall cloud-init [ 0.0] Examining the guest ... [ 11.5] Setting a random seed [ 11.5] Uninstalling packages: cloud-init [ 13.9] Setting passwords [ 15.6] Finishing off On successful completion, copy the image back to the automation host (if it was not Linux) or make it available via an HTTP(S) server. Update the following Terraform input variables: * rhel_password = \"<password>\" Download the RHCOS Qcow2 image Select the RHCOS version you need from the OpenShift mirror . Find and download the QEMU qcow2 image gzip file. eg: rhcos-4.4.9-ppc64le-qemu.ppc64le.qcow2.gz Extract the Qcow2 image and place it on the automation host or make it available via an HTTP(S) server.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#automation-host-prerequisites","text":"Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Terraform Providers Git RHCOS and RHEL 8.X Images for OpenShift Download the RHEL Qcow2 image Customize the RHEL Qcow2 image Download the RHCOS Qcow2 image","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#configure-your-firewall","text":"If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443","title":"Configure Your Firewall"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#automation-host-setup","text":"Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux. It's preferable to run this automation code on Linux host since Linux is required for customizing the RHEL image.","title":"Automation Host Setup"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#terraform","text":"Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. Terraform binary on IBM Power ( ppc64le ) is not available for download from Hashicorp website. You can download it from the following link-1 or link-2 or you will need to compile it from source by running: TAG_VERSION=v0.13.5 git clone https://github.com/hashicorp/terraform --branch $TAG $GOPATH/src/github.com/hashicorp/terraform cd $GOPATH/src/github.com/hashicorp/terraform TF_DEV=1 scripts/build.sh Validate using: $GOPATH/bin/terraform version","title":"Terraform"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#terraform-providers","text":"Please follow the guide to build and install the required Terraform providers.","title":"Terraform Providers"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#git","text":"Git : Please refer to the link for instructions on installing Git.","title":"Git"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#rhcos-and-rhel-8x-images-for-openshift","text":"You'll need to have the RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image available on the automation host or via an HTTP(S) server. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes.","title":"RHCOS and RHEL 8.X Images for OpenShift"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#download-the-rhel-qcow2-image","text":"Login to the RedHat portal and click on the Downloads tab. Click on the product Red Hat Enterprise Linux 8 from the list. In the \"Product Variant\" drop-down select \"Red Hat Enterprise Linux for Power, little endian\". In the \"Version\" drop-down select the version of RHEL you want to download. For example, will use the latest as 8.2. In the \"Product Software\" tab, click on \"Download Now\" button adjacent to \"Red Hat Enterprise Linux 8.2 Update KVM Guest Image\". Save the Qcow2 image. If your automation host is not Linux, then you will need to copy the Qcow2 image to a Linux host for the next step.","title":"Download the RHEL Qcow2 image"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#customize-the-rhel-qcow2-image","text":"Customize the Qcow2 image to set a root password and disable the cloud-init service. You will need the 'libguestfs-tools' package installed on the Linux machine to run the below command (Not available on Mac/Windows). # virt-customize -a <qcow2 image file name> --root-password password:<password> --uninstall cloud-init [ 0.0] Examining the guest ... [ 11.5] Setting a random seed [ 11.5] Uninstalling packages: cloud-init [ 13.9] Setting passwords [ 15.6] Finishing off On successful completion, copy the image back to the automation host (if it was not Linux) or make it available via an HTTP(S) server. Update the following Terraform input variables: * rhel_password = \"<password>\"","title":"Customize the RHEL Qcow2 image"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#download-the-rhcos-qcow2-image","text":"Select the RHCOS version you need from the OpenShift mirror . Find and download the QEMU qcow2 image gzip file. eg: rhcos-4.4.9-ppc64le-qemu.ppc64le.qcow2.gz Extract the Qcow2 image and place it on the automation host or make it available via an HTTP(S) server.","title":"Download the RHCOS Qcow2 image"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/","text":"Libvirt Host Setup Libvirt Host Setup Libvirt Configuration Install and Configure Libvirt Enable IP Forwarding Accept TCP connections Allow Firewall for libvirt connections Password less access Verification Libvirt Configuration The following steps are required to configure Libvirt to work with the automation code Install and Configure Libvirt Please follow the steps given at Install and Enable section Enable IP Forwarding Please follow the steps given at IP forwarding section Accept TCP connections Please follow the steps given at Accept TCP connections section Allow Firewall for libvirt connections Please follow the steps given at Firewalld section Password less access Follow below steps to add the public key of the Terraform client machine user to the authorized list for password-less access. 1. Copy ~/.ssh/id_rsa.pub contents from Terraform client machine. 2. Append the public key as copied above to ~/.ssh/authorized_keys on the KVM host. Verification You can verify TCP connections to the kvm host using an example virsh command given below. virsh --connect qemu+tcp://<host_name_or_ip>/system --readonly","title":"Libvirt Host Setup"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#libvirt-host-setup","text":"Libvirt Host Setup Libvirt Configuration Install and Configure Libvirt Enable IP Forwarding Accept TCP connections Allow Firewall for libvirt connections Password less access Verification","title":"Libvirt Host Setup"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#libvirt-configuration","text":"The following steps are required to configure Libvirt to work with the automation code","title":"Libvirt Configuration"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#install-and-configure-libvirt","text":"Please follow the steps given at Install and Enable section","title":"Install and Configure Libvirt"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#enable-ip-forwarding","text":"Please follow the steps given at IP forwarding section","title":"Enable IP Forwarding"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#accept-tcp-connections","text":"Please follow the steps given at Accept TCP connections section","title":"Accept TCP connections"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#allow-firewall-for-libvirt-connections","text":"Please follow the steps given at Firewalld section","title":"Allow Firewall for libvirt connections"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#password-less-access","text":"Follow below steps to add the public key of the Terraform client machine user to the authorized list for password-less access. 1. Copy ~/.ssh/id_rsa.pub contents from Terraform client machine. 2. Append the public key as copied above to ~/.ssh/authorized_keys on the KVM host.","title":"Password less access"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#verification","text":"You can verify TCP connections to the kvm host using an example virsh command given below. virsh --connect qemu+tcp://<host_name_or_ip>/system --readonly","title":"Verification"},{"location":"ocp4-upi-kvm/docs/quickstart/","text":"Installation Quickstart Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up Download the Automation Code You'll need to use git to clone the deployment code when working off the master branch git clone https://github.com/ocp-power-automation/ocp4-upi-kvm.git cd ocp4_upi_kvm All further instructions assumes you are in the code directory eg. ocp4-upi-kvm Setup Terraform Variables Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history Start Install Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 bootstrap_ip = 192.168.61.3 cluster_id = test-cluster-9a4f etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.61.4\", \"192.168.61.5\", \"192.168.61.6\", ] oc_server_url = https://api.test-cluster-9a4f.mydomain.com:6443/ storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output Post Install Delete Bootstrap Node Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = { memory = 8192, vcpu = 4, count = 0 } Run command terraform apply -var-file var.tfvars Create API and Ingress DNS Records You can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_ip> *.apps.<cluster_id>. IN A <bastion_ip> You'll need bastion_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_ip = 192.168.61.2 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 192.168.61.2 *.apps.test-cluster-9a4f. IN A 192.168.61.2 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_ip> api.<cluster_id> <bastion_ip> console-openshift-console.apps.<cluster_id> <bastion_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_ip> oauth-openshift.apps.<cluster_id> <bastion_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ``` Cluster Access OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@1192.168.61.2:~/openstack-upi/auth/\\* . Using CLI OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config . Using Web UI The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file. Clean up To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Installation Quickstart"},{"location":"ocp4-upi-kvm/docs/quickstart/#installation-quickstart","text":"Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up","title":"Installation Quickstart"},{"location":"ocp4-upi-kvm/docs/quickstart/#download-the-automation-code","text":"You'll need to use git to clone the deployment code when working off the master branch git clone https://github.com/ocp-power-automation/ocp4-upi-kvm.git cd ocp4_upi_kvm All further instructions assumes you are in the code directory eg. ocp4-upi-kvm","title":"Download the Automation Code"},{"location":"ocp4-upi-kvm/docs/quickstart/#setup-terraform-variables","text":"Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history","title":"Setup Terraform Variables"},{"location":"ocp4-upi-kvm/docs/quickstart/#start-install","text":"Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 bootstrap_ip = 192.168.61.3 cluster_id = test-cluster-9a4f etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.61.4\", \"192.168.61.5\", \"192.168.61.6\", ] oc_server_url = https://api.test-cluster-9a4f.mydomain.com:6443/ storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output","title":"Start Install"},{"location":"ocp4-upi-kvm/docs/quickstart/#post-install","text":"","title":"Post Install"},{"location":"ocp4-upi-kvm/docs/quickstart/#delete-bootstrap-node","text":"Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = { memory = 8192, vcpu = 4, count = 0 } Run command terraform apply -var-file var.tfvars","title":"Delete Bootstrap Node"},{"location":"ocp4-upi-kvm/docs/quickstart/#create-api-and-ingress-dns-records","text":"You can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_ip> *.apps.<cluster_id>. IN A <bastion_ip> You'll need bastion_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_ip = 192.168.61.2 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 192.168.61.2 *.apps.test-cluster-9a4f. IN A 192.168.61.2 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_ip> api.<cluster_id> <bastion_ip> console-openshift-console.apps.<cluster_id> <bastion_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_ip> oauth-openshift.apps.<cluster_id> <bastion_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ```","title":"Create API and Ingress DNS Records"},{"location":"ocp4-upi-kvm/docs/quickstart/#cluster-access","text":"OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@1192.168.61.2:~/openstack-upi/auth/\\* .","title":"Cluster Access"},{"location":"ocp4-upi-kvm/docs/quickstart/#using-cli","text":"OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config .","title":"Using CLI"},{"location":"ocp4-upi-kvm/docs/quickstart/#using-web-ui","text":"The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file.","title":"Using Web UI"},{"location":"ocp4-upi-kvm/docs/quickstart/#clean-up","text":"To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Clean up"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/","text":"Terraform Providers At present Terraform registry does not support some plugins. Third-party providers can be manually installed using local filesystem as a mirror . This is in addition to the provider plugins that are downloaded by Terraform during terraform init . Follow below steps based on your Terraform client machine to setup terraform providers. These steps are required to be followed before running the automation. Most of the steps require Go to be installed from https://golang.org/dl/. We recommed Go version above 1.14. Make sure to set your GOPATH environment variable and add $GOPATH/bin to PATH. On Mac/Linux Identify your platform. All example commands assume Linux as a platform: Linux: linux_amd64 Mac OSX: darwin_amd64 Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Mac OSX: ~/Library/Application Support/io.terraform/plugins OR /Library/Application Support/io.terraform/plugins Libvirt provider : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . Run below commands to install libvirt provider. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. PLATFORM=linux_amd64 PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install On IBM Power Systems Identify your platform. All example commands assume Linux as a platform: Linux: linux_ppc64le Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. terraform-provider-libvirt : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt terraform-provider-random : Please refer to the section below for instructions on installing the random provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.3.0 VERSION=2.3.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the random provider plugin: git clone https://github.com/hashicorp/terraform-provider-random --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-random cd $GOPATH/src/github.com/hashicorp/terraform-provider-random make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-random $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/terraform-provider-random terraform-provider-ignition : Please refer to the section below for instructions on installing the ignition provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.0 VERSION=2.1.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the ignition provider plugin: git clone https://github.com/community-terraform-providers/terraform-provider-ignition --branch v$VERSION $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition cd $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-ignition $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/terraform-provider-ignition terraform-provider-null : Please refer to the section below for instructions on installing the null provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.2 VERSION=2.1.2 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the null provider plugin: git clone https://github.com/hashicorp/terraform-provider-null --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-null cd $GOPATH/src/github.com/hashicorp/terraform-provider-null make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-null $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/terraform-provider-null Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install","title":"Terraform Providers"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/#terraform-providers","text":"At present Terraform registry does not support some plugins. Third-party providers can be manually installed using local filesystem as a mirror . This is in addition to the provider plugins that are downloaded by Terraform during terraform init . Follow below steps based on your Terraform client machine to setup terraform providers. These steps are required to be followed before running the automation. Most of the steps require Go to be installed from https://golang.org/dl/. We recommed Go version above 1.14. Make sure to set your GOPATH environment variable and add $GOPATH/bin to PATH.","title":"Terraform Providers"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/#on-maclinux","text":"Identify your platform. All example commands assume Linux as a platform: Linux: linux_amd64 Mac OSX: darwin_amd64 Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Mac OSX: ~/Library/Application Support/io.terraform/plugins OR /Library/Application Support/io.terraform/plugins Libvirt provider : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . Run below commands to install libvirt provider. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. PLATFORM=linux_amd64 PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install","title":"On Mac/Linux"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/#on-ibm-power-systems","text":"Identify your platform. All example commands assume Linux as a platform: Linux: linux_ppc64le Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. terraform-provider-libvirt : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt terraform-provider-random : Please refer to the section below for instructions on installing the random provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.3.0 VERSION=2.3.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the random provider plugin: git clone https://github.com/hashicorp/terraform-provider-random --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-random cd $GOPATH/src/github.com/hashicorp/terraform-provider-random make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-random $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/terraform-provider-random terraform-provider-ignition : Please refer to the section below for instructions on installing the ignition provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.0 VERSION=2.1.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the ignition provider plugin: git clone https://github.com/community-terraform-providers/terraform-provider-ignition --branch v$VERSION $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition cd $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-ignition $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/terraform-provider-ignition terraform-provider-null : Please refer to the section below for instructions on installing the null provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.2 VERSION=2.1.2 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the null provider plugin: git clone https://github.com/hashicorp/terraform-provider-null --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-null cd $GOPATH/src/github.com/hashicorp/terraform-provider-null make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-null $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/terraform-provider-null Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install","title":"On IBM Power Systems"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/","text":"How to use var.tfvars How to use var.tfvars Introduction Libvirt Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations Introduction This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf Libvirt Details These set of variables specify the Libvirt details. libvirt_uri = \"qemu+tcp://localhost/system\" host_address = \"\" images_path = \"/home/libvirt/openshift-images\" OpenShift Cluster Details These set of variables specify the cluster capacity. bastion = { memory = 8192, vcpu = 2 } bootstrap = { memory = 8192, vcpu = 4, count = 1 } master = { memory = 16384, vcpu = 4, count = 3 } worker = { memory = 16384, vcpu = 4, count = 2 } You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. These set of variables specify the RHEL and RHCOS boot image location. Ensure that you use the correct RHCOS image specific to the pre-release version. bastion_image = \"<url-or-path-to-rhel-qcow2>\" rhcos_image = \"<url-or-path-to-rhcos-qcow2>\" These set of variables specify the username, password and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" rhel_password = \"<password>\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" If you have an org wide activation key, then use the following variables rhel_subscription_org = \"\" rhel_subscription_activationkey = \"\" OpenShift Installation Details These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters. Misc Customizations These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variable is used to specify the Libvirt VM CPU mode. For example custom, host-passthrough, host-model. cpu_mode = \"\" The following variable is used to define the network subnet for the OCP cluster. Default is set to '192.168.27.0/24'. network_cidr = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" GThese variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_pause_time = \"90\" upgrade_delay_time = \"600\"","title":"How to use var.tfvars"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#how-to-use-vartfvars","text":"How to use var.tfvars Introduction Libvirt Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations","title":"How to use var.tfvars"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#introduction","text":"This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf","title":"Introduction"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#libvirt-details","text":"These set of variables specify the Libvirt details. libvirt_uri = \"qemu+tcp://localhost/system\" host_address = \"\" images_path = \"/home/libvirt/openshift-images\"","title":"Libvirt Details"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#openshift-cluster-details","text":"These set of variables specify the cluster capacity. bastion = { memory = 8192, vcpu = 2 } bootstrap = { memory = 8192, vcpu = 4, count = 1 } master = { memory = 16384, vcpu = 4, count = 3 } worker = { memory = 16384, vcpu = 4, count = 2 } You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. These set of variables specify the RHEL and RHCOS boot image location. Ensure that you use the correct RHCOS image specific to the pre-release version. bastion_image = \"<url-or-path-to-rhel-qcow2>\" rhcos_image = \"<url-or-path-to-rhcos-qcow2>\" These set of variables specify the username, password and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" rhel_password = \"<password>\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" If you have an org wide activation key, then use the following variables rhel_subscription_org = \"\" rhel_subscription_activationkey = \"\"","title":"OpenShift Cluster Details"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#openshift-installation-details","text":"These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters.","title":"OpenShift Installation Details"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#misc-customizations","text":"These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variable is used to specify the Libvirt VM CPU mode. For example custom, host-passthrough, host-model. cpu_mode = \"\" The following variable is used to define the network subnet for the OCP cluster. Default is set to '192.168.27.0/24'. network_cidr = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" GThese variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_pause_time = \"90\" upgrade_delay_time = \"600\"","title":"Misc Customizations"},{"location":"ocp4-upi-powervm/","text":"Table of Contents Table of Contents Introduction Automation Host Prerequisites PowerVC Prerequisites OCP Install Contributing Introduction The ocp4-upi-powervm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on PowerVM systems managed by PowerVC. If you are using standalone PowerVM please take a look at the following quickstart guide which uses the ansible playbook to setup a helper node (bastion) for OCP deployment. This project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC. Note For bugs/enhancement requests etc. please open a GitHub issue Warning The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches -{ release-4.5 , release-4.6 ...} and follow the docs in the specific release branches. Automation Host Prerequisites The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Mac OSX (Darwin) - Linux (x86_64/ppc64le) - Windows 10 Follow the guide to complete the prerequisites. PowerVC Prerequisites Follow the guide to complete the PowerVC prerequisites. OCP Install Follow the quickstart guide for OCP installation on PowerVM LPARs managed via PowerVC Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"Deploying OpenShift on PowerVM managed via PowerVC"},{"location":"ocp4-upi-powervm/#table-of-contents","text":"Table of Contents Introduction Automation Host Prerequisites PowerVC Prerequisites OCP Install Contributing","title":"Table of Contents"},{"location":"ocp4-upi-powervm/#introduction","text":"The ocp4-upi-powervm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on PowerVM systems managed by PowerVC. If you are using standalone PowerVM please take a look at the following quickstart guide which uses the ansible playbook to setup a helper node (bastion) for OCP deployment. This project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC. Note For bugs/enhancement requests etc. please open a GitHub issue Warning The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches -{ release-4.5 , release-4.6 ...} and follow the docs in the specific release branches.","title":"Introduction"},{"location":"ocp4-upi-powervm/#automation-host-prerequisites","text":"The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Mac OSX (Darwin) - Linux (x86_64/ppc64le) - Windows 10 Follow the guide to complete the prerequisites.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervm/#powervc-prerequisites","text":"Follow the guide to complete the PowerVC prerequisites.","title":"PowerVC Prerequisites"},{"location":"ocp4-upi-powervm/#ocp-install","text":"Follow the quickstart guide for OCP installation on PowerVM LPARs managed via PowerVC","title":"OCP Install"},{"location":"ocp4-upi-powervm/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Attribution"},{"location":"ocp4-upi-powervm/CONTRIBUTING/","text":"Contributing This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project. Issues If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed. Pull Request Process To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s . Spec Formatting Conventions Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Contributing"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#contributing","text":"This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#issues","text":"If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed.","title":"Issues"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#pull-request-process","text":"To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Pull Request Process"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#spec-formatting-conventions","text":"Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Spec Formatting Conventions"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/","text":"Automation Host Prerequisites Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Git Configure Your Firewall If your system is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443 Automation Host Setup Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux/Windows. Terraform Terraform : Please open the link for downloading the latest Terraform. For validating the version run terraform version command after install. Terraform version 1.2.0 and above is required. Install Terraform and providers for Power environment: 1. Download and install the latest Terraform binary for Linux/ppc64le from https://github.com/ppc64le-development/terraform-ppc64le/releases. 2. Download the required Terraform providers for Power into your TF project directory: $ cd <path_to_TF_project> $ mkdir -p ./providers $ curl -fsSL https://github.com/ocp-power-automation/terraform-providers-power/releases/download/v0.11/archive.zip -o archive.zip $ unzip -o ./archive.zip -d ./providers $ rm -f ./archive.zip Initialize Terraform at your TF project directory: $ terraform init --plugin-dir ./providers Git Git : Please refer to the link for instructions on installing Git.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#automation-host-prerequisites","text":"Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Git","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#configure-your-firewall","text":"If your system is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443","title":"Configure Your Firewall"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#automation-host-setup","text":"Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux/Windows.","title":"Automation Host Setup"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#terraform","text":"Terraform : Please open the link for downloading the latest Terraform. For validating the version run terraform version command after install. Terraform version 1.2.0 and above is required. Install Terraform and providers for Power environment: 1. Download and install the latest Terraform binary for Linux/ppc64le from https://github.com/ppc64le-development/terraform-ppc64le/releases. 2. Download the required Terraform providers for Power into your TF project directory: $ cd <path_to_TF_project> $ mkdir -p ./providers $ curl -fsSL https://github.com/ocp-power-automation/terraform-providers-power/releases/download/v0.11/archive.zip -o archive.zip $ unzip -o ./archive.zip -d ./providers $ rm -f ./archive.zip Initialize Terraform at your TF project directory: $ terraform init --plugin-dir ./providers","title":"Terraform"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#git","text":"Git : Please refer to the link for instructions on installing Git.","title":"Git"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/","text":"Create Master/Worker/Bootstrap Nodes Create the below PowerVM LPARS with empty disk volume attached (refer the documentation link for resource requirements) and note the MAC ID for each of the LPARs. bootstrap - 1 master - 3 worker - 2 Create and Setup Bastion Node Create RHEL 8.1 LPAR Login to the RHEL 8.1 LPAR and clone the OCP4 helpernode repo Use the following vars.yaml as a template and change the IP, network and related details according to your environment. --- disk: sda helper: name: \"helper\" ipaddr: \"192.168.7.77\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" ppc64le: true ocp_bios: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-metal.ppc64le.raw.gz\" ocp_initramfs: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-initramfs.ppc64le.img\" ocp_install_kernel: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-kernel-ppc64le\" ocp_client: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-client-linux.tar.gz\" ocp_installer: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-install-linux.tar.gz\" Run the playbook ansible-playbook -e @vars.yaml tasks/main.yml Create ignition configs mkdir ~/ocp4 cd ~/ocp4 Create a place to store your pull-secret mkdir -p ~/.openshift Visit try.openshift.com and select \"Bare Metal\". Download your pull secret and save it under ~/.openshift/pull-secret # ls -1 ~/.openshift/pull-secret /root/.openshift/pull-secret This playbook creates an ssh key for you; it's under ~/.ssh/helper_rsa . You can use this key or create/user another one if you wish. # ls -1 ~/.ssh/helper_rsa /root/.ssh/helper_rsa Note - If you want you use your own ssh key, please modify ~/.ssh/config to reference your key instead of the one deployed by the playbook Next, create an install-config.yaml file. Note - Make sure you update if your filenames or paths are different. cat <<EOF > install-config.yaml apiVersion: v1 baseDomain: example.com compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp4 networking: clusterNetworks: - cidr: 10.254.0.0/16 hostPrefix: 24 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} pullSecret: '$(< ~/.openshift/pull-secret)' sshKey: '$(< ~/.ssh/helper_rsa.pub)' EOF Create the installation manifests openshift-install create manifests Edit the manifests/cluster-scheduler-02-config.yml Kubernetes manifest file to prevent Pods from being scheduled on the control plane machines by setting mastersSchedulable to false . $ sed -i 's/mastersSchedulable: true/mastersSchedulable: false/g' manifests/cluster-scheduler-02-config.yml It should look something like this after you edit it. $ cat manifests/cluster-scheduler-02-config.yml apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false policy: name: \"\" status: {} Next, generate the ignition configs openshift-install create ignition-configs Finally, copy the ignition files in the ignition directory for the websever cp ~/ocp4/*.ign /var/www/html/ignition/ restorecon -vR /var/www/html/ chmod o+r /var/www/html/ignition/*.ign Boot the LPARs Boot the LPARs in the following order and ensure the LPARs perform DHCP boot Bootstrap Masters Workers Wait for Install openshift-install wait-for bootstrap-complete --log-level debug Finish Install First, login to your cluster export KUBECONFIG=/root/ocp4/auth/kubeconfig Your install may be waiting for worker nodes to get approved. Normally it's automated. However, sometimes this needs to be done manually. Check pending CSRs with the following command. oc get csr You can approve all pending CSRs in \"one shot\" with the following command oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve You may have to run the command multiple times depending on how many workers you have and in what order they come in. Keep a watch on the CSRs by running the following command watch oc get csr Set the registry for your cluster First, you have to set the managementState to Managed for your cluster oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}' For PoCs, using emptyDir is ok (to use PVs follow this doc) oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"storage\":{\"emptyDir\":{}}}}' If you need to expose the registry, run this command oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p '{\"spec\":{\"defaultRoute\":true}}' Note - You can watch the operators running with oc get clusteroperators Login to the web console The OpenShift 4 web console will be running at https://console-openshift-console.apps.{{ dns.clusterid }}.{{ dns.domain }} (e.g. https://console-openshift-console.apps.ocp4.example.com) Username: kubeadmin Password: the output of cat /root/ocp4/auth/kubeadmin-password Note - You'll need to update your /etc/hosts settings if using private dhcp server running on the bastion node References: - Quickstart Guide - Power QuickStart Guide","title":"Ocp4 manual deployment dhcp"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#create-masterworkerbootstrap-nodes","text":"Create the below PowerVM LPARS with empty disk volume attached (refer the documentation link for resource requirements) and note the MAC ID for each of the LPARs. bootstrap - 1 master - 3 worker - 2","title":"Create Master/Worker/Bootstrap Nodes"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#create-and-setup-bastion-node","text":"Create RHEL 8.1 LPAR Login to the RHEL 8.1 LPAR and clone the OCP4 helpernode repo Use the following vars.yaml as a template and change the IP, network and related details according to your environment. --- disk: sda helper: name: \"helper\" ipaddr: \"192.168.7.77\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" ppc64le: true ocp_bios: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-metal.ppc64le.raw.gz\" ocp_initramfs: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-initramfs.ppc64le.img\" ocp_install_kernel: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-kernel-ppc64le\" ocp_client: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-client-linux.tar.gz\" ocp_installer: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-install-linux.tar.gz\" Run the playbook ansible-playbook -e @vars.yaml tasks/main.yml Create ignition configs mkdir ~/ocp4 cd ~/ocp4 Create a place to store your pull-secret mkdir -p ~/.openshift Visit try.openshift.com and select \"Bare Metal\". Download your pull secret and save it under ~/.openshift/pull-secret # ls -1 ~/.openshift/pull-secret /root/.openshift/pull-secret This playbook creates an ssh key for you; it's under ~/.ssh/helper_rsa . You can use this key or create/user another one if you wish. # ls -1 ~/.ssh/helper_rsa /root/.ssh/helper_rsa Note - If you want you use your own ssh key, please modify ~/.ssh/config to reference your key instead of the one deployed by the playbook Next, create an install-config.yaml file. Note - Make sure you update if your filenames or paths are different. cat <<EOF > install-config.yaml apiVersion: v1 baseDomain: example.com compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp4 networking: clusterNetworks: - cidr: 10.254.0.0/16 hostPrefix: 24 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} pullSecret: '$(< ~/.openshift/pull-secret)' sshKey: '$(< ~/.ssh/helper_rsa.pub)' EOF Create the installation manifests openshift-install create manifests Edit the manifests/cluster-scheduler-02-config.yml Kubernetes manifest file to prevent Pods from being scheduled on the control plane machines by setting mastersSchedulable to false . $ sed -i 's/mastersSchedulable: true/mastersSchedulable: false/g' manifests/cluster-scheduler-02-config.yml It should look something like this after you edit it. $ cat manifests/cluster-scheduler-02-config.yml apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false policy: name: \"\" status: {} Next, generate the ignition configs openshift-install create ignition-configs Finally, copy the ignition files in the ignition directory for the websever cp ~/ocp4/*.ign /var/www/html/ignition/ restorecon -vR /var/www/html/ chmod o+r /var/www/html/ignition/*.ign","title":"Create and Setup Bastion Node"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#boot-the-lpars","text":"Boot the LPARs in the following order and ensure the LPARs perform DHCP boot Bootstrap Masters Workers","title":"Boot the LPARs"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#wait-for-install","text":"openshift-install wait-for bootstrap-complete --log-level debug","title":"Wait for Install"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#finish-install","text":"First, login to your cluster export KUBECONFIG=/root/ocp4/auth/kubeconfig Your install may be waiting for worker nodes to get approved. Normally it's automated. However, sometimes this needs to be done manually. Check pending CSRs with the following command. oc get csr You can approve all pending CSRs in \"one shot\" with the following command oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve You may have to run the command multiple times depending on how many workers you have and in what order they come in. Keep a watch on the CSRs by running the following command watch oc get csr Set the registry for your cluster First, you have to set the managementState to Managed for your cluster oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}' For PoCs, using emptyDir is ok (to use PVs follow this doc) oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"storage\":{\"emptyDir\":{}}}}' If you need to expose the registry, run this command oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p '{\"spec\":{\"defaultRoute\":true}}' Note - You can watch the operators running with oc get clusteroperators","title":"Finish Install"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#login-to-the-web-console","text":"The OpenShift 4 web console will be running at https://console-openshift-console.apps.{{ dns.clusterid }}.{{ dns.domain }} (e.g. https://console-openshift-console.apps.ocp4.example.com) Username: kubeadmin Password: the output of cat /root/ocp4/auth/kubeadmin-password Note - You'll need to update your /etc/hosts settings if using private dhcp server running on the bastion node References: - Quickstart Guide - Power QuickStart Guide","title":"Login to the web console"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/","text":"PowerVC Prerequisites RHCOS and RHEL 8.X Images for OpenShift You'll need to create RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image in PowerVC. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes. For RHCOS image creation, follow the steps mentioned in the following doc . For RHEL image creation follow the steps mentioned in the following doc , you may either create a new image from ISO, or use a similar method like CoreOS with a qcow2 image. Compute Templates You'll need to create compute templates for bastion, bootstrap, master and worker nodes. Following are the recommended LPAR configs that you can use when creating the compute templates for different type of nodes Bootstrap - 2 vCPUs, 16GB RAM, 120 GB Disk. Master - 2 vCPUs, 32GB RAM, 120 GB Disk. PowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be 16 ( 2 vCPUs x 8 SMT ) This config is suitable for majority of the scenarios Worker - 2 vCPUs, 32GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Bastion - 2vCPUs, 16GB RAM, 200 GB Disk Increase bastion vCPUs, RAM and Disk based on application requirements","title":"**PowerVC Prerequisites**"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/#powervc-prerequisites","text":"","title":"PowerVC Prerequisites"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/#rhcos-and-rhel-8x-images-for-openshift","text":"You'll need to create RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image in PowerVC. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes. For RHCOS image creation, follow the steps mentioned in the following doc . For RHEL image creation follow the steps mentioned in the following doc , you may either create a new image from ISO, or use a similar method like CoreOS with a qcow2 image.","title":"RHCOS and RHEL 8.X Images for OpenShift"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/#compute-templates","text":"You'll need to create compute templates for bastion, bootstrap, master and worker nodes. Following are the recommended LPAR configs that you can use when creating the compute templates for different type of nodes Bootstrap - 2 vCPUs, 16GB RAM, 120 GB Disk. Master - 2 vCPUs, 32GB RAM, 120 GB Disk. PowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be 16 ( 2 vCPUs x 8 SMT ) This config is suitable for majority of the scenarios Worker - 2 vCPUs, 32GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Bastion - 2vCPUs, 16GB RAM, 200 GB Disk Increase bastion vCPUs, RAM and Disk based on application requirements","title":"Compute Templates"},{"location":"ocp4-upi-powervm/docs/quickstart/","text":"Installation Quickstart Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up Download the Automation Code You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervm.git $ cd ocp4_upi_powervm All further instructions assumes you are in the code directory eg. ocp4-upi-powervm Setup Terraform Variables Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export POWERVC_USERNAME=xxxxxxxxxxxxxxx $ export POWERVC_PASSWORD=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history Start Install Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var user_name=\"$POWERVC_USERNAME\" -var password=\"$POWERVC_PASSWORD\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds. Post Install Delete Bootstrap Node Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {instance_type = \"medium\", image_id = \"468863e6-4b33-4e8b-b2c5-c9ef9e6eedf4\", \"count\" = 0} Run command terraform apply -var-file var.tfvars Create API and Ingress DNS Records Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_public_ip> *.apps.<cluster_id>. IN A <bastion_public_ip> You'll need bastion_public_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_public_ip = 16.20.34.5 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 16.20.34.5 *.apps.test-cluster-9a4f. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_public_ip> api.<cluster_id> <bastion_public_ip> console-openshift-console.apps.<cluster_id> <bastion_public_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_public_ip> oauth-openshift.apps.<cluster_id> <bastion_public_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ``` Cluster Access OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* . Using CLI OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config . Using Web UI The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file. Clean up To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Installation Quickstart"},{"location":"ocp4-upi-powervm/docs/quickstart/#installation-quickstart","text":"Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up","title":"Installation Quickstart"},{"location":"ocp4-upi-powervm/docs/quickstart/#download-the-automation-code","text":"You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervm.git $ cd ocp4_upi_powervm All further instructions assumes you are in the code directory eg. ocp4-upi-powervm","title":"Download the Automation Code"},{"location":"ocp4-upi-powervm/docs/quickstart/#setup-terraform-variables","text":"Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export POWERVC_USERNAME=xxxxxxxxxxxxxxx $ export POWERVC_PASSWORD=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history","title":"Setup Terraform Variables"},{"location":"ocp4-upi-powervm/docs/quickstart/#start-install","text":"Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var user_name=\"$POWERVC_USERNAME\" -var password=\"$POWERVC_PASSWORD\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds.","title":"Start Install"},{"location":"ocp4-upi-powervm/docs/quickstart/#post-install","text":"","title":"Post Install"},{"location":"ocp4-upi-powervm/docs/quickstart/#delete-bootstrap-node","text":"Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {instance_type = \"medium\", image_id = \"468863e6-4b33-4e8b-b2c5-c9ef9e6eedf4\", \"count\" = 0} Run command terraform apply -var-file var.tfvars","title":"Delete Bootstrap Node"},{"location":"ocp4-upi-powervm/docs/quickstart/#create-api-and-ingress-dns-records","text":"Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_public_ip> *.apps.<cluster_id>. IN A <bastion_public_ip> You'll need bastion_public_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_public_ip = 16.20.34.5 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 16.20.34.5 *.apps.test-cluster-9a4f. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_public_ip> api.<cluster_id> <bastion_public_ip> console-openshift-console.apps.<cluster_id> <bastion_public_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_public_ip> oauth-openshift.apps.<cluster_id> <bastion_public_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ```","title":"Create API and Ingress DNS Records"},{"location":"ocp4-upi-powervm/docs/quickstart/#cluster-access","text":"OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* .","title":"Cluster Access"},{"location":"ocp4-upi-powervm/docs/quickstart/#using-cli","text":"OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config .","title":"Using CLI"},{"location":"ocp4-upi-powervm/docs/quickstart/#using-web-ui","text":"The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file.","title":"Using Web UI"},{"location":"ocp4-upi-powervm/docs/quickstart/#clean-up","text":"To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Clean up"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/","text":"Introduction Option-1 Option-2 Introduction Depending on your environment you can follow one of the options to create RHCOS (CoreOS) image in PowerVC Option-1 Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM having an additional empty volume with minimum size of 120G. Please make a note of the new volume name . Login to the VM and execute the following steps Install wget , qemu-img , parted and gzip packages Transfer the downloaded RHCOS image to this VM Extract the image $ gunzip rhcos-openstack.ppc64le.qcow2.gz Convert the CoreOS qcow2 image to raw image $ qemu-img convert -f qcow2 -O raw rhcos-openstack.ppc64le.qcow2 rhcos-latest.raw Identify the disk device representing the additional empty volume attached to the VM $ disk_device_list=$(sudo parted -l 2>&1 | grep -E -v \"$readonly\" | grep -E -i \"ERROR:\" |cut -f2 -d: | grep -v \"Can't\" | xargs -i echo \"Disk.{}:|\" | xargs echo | tr -d ' ' | rev | cut -c2- | rev) $ empty_disk_device=$(sudo fdisk -l | grep -E \"$disk_list\" | sort -k5nr | head -n 1 | tail -n1 | cut -f1 -d: | cut -f2 -d' ') $ echo \"$empty_disk_device\"' Dump the raw image to the newly added disk $ dd if=rhcos-latest.raw of=${empty_disk_device} bs=4M where ${empty_disk_device} is the device representing the attached volume Detach the volume, from the VM Go to PowerVC UI->images and select create for creating a new image Specify image name and choose PowerVM for Hypervisor type, RHEL for Operating system and littleEndian for Endianness Select Add Volume and search for the specific volume name (where you dd-ed the RHCOS image ) and set Boot set to yes. Create the image by clicking on create Option-2 Creating and importing RHCOS OVA image Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM Use the script https://github.com/ocp-power-automation/infra/blob/master/scripts/images/convert_qcow2_ova.py and convert the RHCOS qcow2 image to an OVA formatted image. Follow the steps mentioned in PowerVC docs to import the OVA image.","title":"Rhcos image creation"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/#introduction","text":"Depending on your environment you can follow one of the options to create RHCOS (CoreOS) image in PowerVC","title":"Introduction"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/#option-1","text":"Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM having an additional empty volume with minimum size of 120G. Please make a note of the new volume name . Login to the VM and execute the following steps Install wget , qemu-img , parted and gzip packages Transfer the downloaded RHCOS image to this VM Extract the image $ gunzip rhcos-openstack.ppc64le.qcow2.gz Convert the CoreOS qcow2 image to raw image $ qemu-img convert -f qcow2 -O raw rhcos-openstack.ppc64le.qcow2 rhcos-latest.raw Identify the disk device representing the additional empty volume attached to the VM $ disk_device_list=$(sudo parted -l 2>&1 | grep -E -v \"$readonly\" | grep -E -i \"ERROR:\" |cut -f2 -d: | grep -v \"Can't\" | xargs -i echo \"Disk.{}:|\" | xargs echo | tr -d ' ' | rev | cut -c2- | rev) $ empty_disk_device=$(sudo fdisk -l | grep -E \"$disk_list\" | sort -k5nr | head -n 1 | tail -n1 | cut -f1 -d: | cut -f2 -d' ') $ echo \"$empty_disk_device\"' Dump the raw image to the newly added disk $ dd if=rhcos-latest.raw of=${empty_disk_device} bs=4M where ${empty_disk_device} is the device representing the attached volume Detach the volume, from the VM Go to PowerVC UI->images and select create for creating a new image Specify image name and choose PowerVM for Hypervisor type, RHEL for Operating system and littleEndian for Endianness Select Add Volume and search for the specific volume name (where you dd-ed the RHCOS image ) and set Boot set to yes. Create the image by clicking on create","title":"Option-1"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/#option-2","text":"Creating and importing RHCOS OVA image Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM Use the script https://github.com/ocp-power-automation/infra/blob/master/scripts/images/convert_qcow2_ova.py and convert the RHCOS qcow2 image to an OVA formatted image. Follow the steps mentioned in PowerVC docs to import the OVA image.","title":"Option-2"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/","text":"How to use var.tfvars How to use var.tfvars Introduction PowerVC Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations Introduction This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf PowerVC Details These set of variables specify the PowerVC details. auth_url = \"<https://<HOSTNAME>:5000/v3/>\" user_name = \"<powervc-login-user-name>\" password = \"<powervc-login-user-password>\" tenant_name = \"<tenant_name>\" domain_name = \"Default\" This variable specifies the network that will be used by the VMs network_name = \"<network_name>\" This variable specifies the availability zone (PowerVC Host Group) in which to create the VMs. Leave it empty to use the \"default\" availability zone. openstack_availability_zone = \"\" OpenShift Cluster Details These set of variables specify the cluster capacity. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 1} bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 1} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2} instance_type is the compute template to be used and image_id is the image UUID. count specifies the number of VMs that should be created for each type. To enable high availability (HA) for cluster services running on the bastion set the bastion count value to 2. Note that in case of HA, the automation will not setup NFS storage. count of 1 for bastion implies the default non-HA bastion setup. You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. availability_zone is an optional attribute for bastion, bootstrap, master and worker. If it is specified, the VM will be created in the specified availability_zone , otherwise value of openstack_availability_zone will be used. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 1} bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", availability_zone = \"\", \"count\" = 1} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", availability_zone = \"master-zone\", \"count\" = 3} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", availability_zone = \"worker-zone\", \"count\" = 2} Above will create the bastion in openstack_availability_zone , bootstrap in default availability zone, masters in master-zone , and workers in worker-zone . To set a pre-defined IPv4 address for the bastion node, make use of the optional fixed_ip_v4 in bastion variable as shown below. Ensure this address is within the given network subnet range and not already in use. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 1, fixed_ip_v4 = \"<IPv4 address>\"} For bastion HA with pre-defined IPs, here the fixed_ip_v4 will be the VIP for bastions: bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 2, fixed_ip_v4 = \"<IPv4 address>\", fixed_ips = [\"<IPv4 address>\", \"<IPv4 address>\"]} To use predefined IPs for bootstrap, master and worker node, use the optional fixed_ips in bootstrap, master and worker variables, number of IPs have to match the count number as shown below: bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 1, fixed_ips = [\"<IPv4 address>\"]} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3, fixed_ips = [\"<IPv4 address>\", \"<IPv4 address>\", \"<IPv4 address>\"]} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2, fixed_ips = [\"<IPv4 address>\", \"<IPv4 address>\"]} To attach additional volumes to master or worker nodes, set the optional data_volume_count key to the number of volumes that is to be attached and the data_volume_size to the size (in GB) for each volume. master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3, data_volume_count = 0, data_volume_size = 100} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2, data_volume_count = 0, data_volume_size = 100} These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" #Set it to an appropriate username for non-root user access public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" rhel_username is set to root. rhel_username can be set to an appropriate username having superuser privileges with no password prompt. Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details, RHEL subscription supports two methods: one is using username and password, the other is using activation key. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" Or define following variables to use activation key for RHEL subscription: rhel_subscription_org = \"org-id\" rhel_subscription_activationkey = \"activation-key\" OpenShift Installation Details These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp If cluster_if_prefix is not set, the cluster_id will be used only without prefix. A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters. FIPS Variable for OpenShift deployment These variables will be used for deploying OCP in FIPS mode. Change the values as per your requirement. fips_compliant = false Misc Customizations These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variables are used to define the IP address for the preconfigured external DNS and the Load-balancer lb_ipaddr = \"\" ext_dns = \"\" The following variable is used to set the network adapter type for the VMs. By default the VMs will use SEA. If SRIOV is required then uncomment the variable network_type = \"SRIOV\" The following variable is used to define the amount of SR-IOV Virtual Functions used for VNIC failover of the network adapter for the VMs. By default the VMs will use 1, which defines no VNIC failover . Any setting higher then 1 creates additional virtual functions and configures them in a VNIC failover setup. Be aware of the fact, that RHCOS and some Linux releases might not handle VNIC failover with more then 2 SR-IOV Virtual Functions properly. The recommended value is 2 for VNIC failover. Valid options are: Any number supported for VNIC failover from 1 to 6 sriov_vnic_failover_vfs = 1 The following variable is used to define the capacity of SR-IOV Logical Ports of the 1st network adapter for the VMs. By default the VMs will use 2%. Valid options are: Any number which can be devided by 2 and results in an integer. 100% = 1.0; 80% = 0.80; 60% = 0.60; etc sriov_capacity = 0.02 The following variable is used to specify the PowerVC Storage Connectivity Group (SCG). Empty value will use the default SCG scg_id = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/RedHatOfficial/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" This variable specify the MTU value for the private network interface on RHEL and RHCOS nodes. The CNI network will have - 50 for OpenshiftSDN and - 100 for OVNKubernetes network provider. private_network_mtu = 1450 These variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of day-1 kernel arguments for the cluster nodes. To add kernel arguments to master or worker nodes, using MachineConfig object and inject that object into the set of manifest files used by Ignition during cluster setup. rhcos_pre_kernel_options = [] Example 1 rhcos_pre_kernel_options = [\"rd.multipath=default\",\"root=/dev/disk/by-label/dm-mpath-root\"] List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = false If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. The following variable allows using RAM disk for etcd. This is not meant for production use cases mount_etcd_ramdisk = false These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_storage_template = \"\" The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_image = \"\" #(e.g. `\"quay.io/openshift-release-dev/ocp-release-nightly@sha256:xxxxx\"`) upgrade_pause_time = \"90\" upgrade_delay_time = \"600\" This variable is used to set the default Container Network Interface (CNI) network provider such as OpenShiftSDN or OVNKubernetes cni_network_provider = \"OpenshiftSDN\" cluster_network_cidr = \"10.128.0.0/14\" cluster_network_hostprefix = \"23\" service_network = \"172.30.0.0/16\"","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#how-to-use-vartfvars","text":"How to use var.tfvars Introduction PowerVC Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#introduction","text":"This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf","title":"Introduction"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#powervc-details","text":"These set of variables specify the PowerVC details. auth_url = \"<https://<HOSTNAME>:5000/v3/>\" user_name = \"<powervc-login-user-name>\" password = \"<powervc-login-user-password>\" tenant_name = \"<tenant_name>\" domain_name = \"Default\" This variable specifies the network that will be used by the VMs network_name = \"<network_name>\" This variable specifies the availability zone (PowerVC Host Group) in which to create the VMs. Leave it empty to use the \"default\" availability zone. openstack_availability_zone = \"\"","title":"PowerVC Details"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#openshift-cluster-details","text":"These set of variables specify the cluster capacity. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 1} bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 1} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2} instance_type is the compute template to be used and image_id is the image UUID. count specifies the number of VMs that should be created for each type. To enable high availability (HA) for cluster services running on the bastion set the bastion count value to 2. Note that in case of HA, the automation will not setup NFS storage. count of 1 for bastion implies the default non-HA bastion setup. You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. availability_zone is an optional attribute for bastion, bootstrap, master and worker. If it is specified, the VM will be created in the specified availability_zone , otherwise value of openstack_availability_zone will be used. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 1} bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", availability_zone = \"\", \"count\" = 1} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", availability_zone = \"master-zone\", \"count\" = 3} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", availability_zone = \"worker-zone\", \"count\" = 2} Above will create the bastion in openstack_availability_zone , bootstrap in default availability zone, masters in master-zone , and workers in worker-zone . To set a pre-defined IPv4 address for the bastion node, make use of the optional fixed_ip_v4 in bastion variable as shown below. Ensure this address is within the given network subnet range and not already in use. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 1, fixed_ip_v4 = \"<IPv4 address>\"} For bastion HA with pre-defined IPs, here the fixed_ip_v4 will be the VIP for bastions: bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\", \"count\" = 2, fixed_ip_v4 = \"<IPv4 address>\", fixed_ips = [\"<IPv4 address>\", \"<IPv4 address>\"]} To use predefined IPs for bootstrap, master and worker node, use the optional fixed_ips in bootstrap, master and worker variables, number of IPs have to match the count number as shown below: bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 1, fixed_ips = [\"<IPv4 address>\"]} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3, fixed_ips = [\"<IPv4 address>\", \"<IPv4 address>\", \"<IPv4 address>\"]} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2, fixed_ips = [\"<IPv4 address>\", \"<IPv4 address>\"]} To attach additional volumes to master or worker nodes, set the optional data_volume_count key to the number of volumes that is to be attached and the data_volume_size to the size (in GB) for each volume. master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3, data_volume_count = 0, data_volume_size = 100} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2, data_volume_count = 0, data_volume_size = 100} These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" #Set it to an appropriate username for non-root user access public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" rhel_username is set to root. rhel_username can be set to an appropriate username having superuser privileges with no password prompt. Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details, RHEL subscription supports two methods: one is using username and password, the other is using activation key. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" Or define following variables to use activation key for RHEL subscription: rhel_subscription_org = \"org-id\" rhel_subscription_activationkey = \"activation-key\"","title":"OpenShift Cluster Details"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#openshift-installation-details","text":"These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp If cluster_if_prefix is not set, the cluster_id will be used only without prefix. A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters.","title":"OpenShift Installation Details"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#fips-variable-for-openshift-deployment","text":"These variables will be used for deploying OCP in FIPS mode. Change the values as per your requirement. fips_compliant = false","title":"FIPS Variable for OpenShift deployment"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#misc-customizations","text":"These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variables are used to define the IP address for the preconfigured external DNS and the Load-balancer lb_ipaddr = \"\" ext_dns = \"\" The following variable is used to set the network adapter type for the VMs. By default the VMs will use SEA. If SRIOV is required then uncomment the variable network_type = \"SRIOV\" The following variable is used to define the amount of SR-IOV Virtual Functions used for VNIC failover of the network adapter for the VMs. By default the VMs will use 1, which defines no VNIC failover . Any setting higher then 1 creates additional virtual functions and configures them in a VNIC failover setup. Be aware of the fact, that RHCOS and some Linux releases might not handle VNIC failover with more then 2 SR-IOV Virtual Functions properly. The recommended value is 2 for VNIC failover. Valid options are: Any number supported for VNIC failover from 1 to 6 sriov_vnic_failover_vfs = 1 The following variable is used to define the capacity of SR-IOV Logical Ports of the 1st network adapter for the VMs. By default the VMs will use 2%. Valid options are: Any number which can be devided by 2 and results in an integer. 100% = 1.0; 80% = 0.80; 60% = 0.60; etc sriov_capacity = 0.02 The following variable is used to specify the PowerVC Storage Connectivity Group (SCG). Empty value will use the default SCG scg_id = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/RedHatOfficial/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" This variable specify the MTU value for the private network interface on RHEL and RHCOS nodes. The CNI network will have - 50 for OpenshiftSDN and - 100 for OVNKubernetes network provider. private_network_mtu = 1450 These variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of day-1 kernel arguments for the cluster nodes. To add kernel arguments to master or worker nodes, using MachineConfig object and inject that object into the set of manifest files used by Ignition during cluster setup. rhcos_pre_kernel_options = [] Example 1 rhcos_pre_kernel_options = [\"rd.multipath=default\",\"root=/dev/disk/by-label/dm-mpath-root\"] List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = false If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. The following variable allows using RAM disk for etcd. This is not meant for production use cases mount_etcd_ramdisk = false These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_storage_template = \"\" The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_image = \"\" #(e.g. `\"quay.io/openshift-release-dev/ocp-release-nightly@sha256:xxxxx\"`) upgrade_pause_time = \"90\" upgrade_delay_time = \"600\" This variable is used to set the default Container Network Interface (CNI) network provider such as OpenShiftSDN or OVNKubernetes cni_network_provider = \"OpenshiftSDN\" cluster_network_cidr = \"10.128.0.0/14\" cluster_network_hostprefix = \"23\" service_network = \"172.30.0.0/16\"","title":"Misc Customizations"},{"location":"ocp4-upi-powervs/","text":"Table of Contents Table of Contents Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install Contributing Introduction The ocp4-upi-powervs project provides Terraform based automation code to help with the deployment of OpenShift Container Platform (OCP) 4.x on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . This project leverages the helpernode ansible playbook internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS). Before You Start The main branch must be used with latest OCP pre-release versions only. Ensure you checkout the correct release-VERSION branch depending on the version (4.5, 4.6 ...) of RedHat OpenShift you want to install: For example to use OCP 4.8 perform the following export VERSION=4.8 git clone --single-branch --branch release-$VERSION https://github.com/ocp-power-automation/ocp4-upi-powervs.git Make It Better For bugs/enhancement requests etc. please open a GitHub issue Getting Started With PowerVS Power Systems Virtual Servers(IBM Cloud Docs) IBM Power Systems in the Multicloud(Youtube video) PowerVS (Youtube video) Automation Host Prerequisites The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems: - Mac OSX (Darwin) - Linux (x86_64/ppc64le) - Windows 10 Follow the guide to complete the prerequisites. PowerVS Prerequisites Follow the guide to complete the PowerVS prerequisites. OCP Install Follow the quickstart guide for OCP installation on PowerVS. Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"Deploying OpenShift on IBM Cloud Power Virtual Servers"},{"location":"ocp4-upi-powervs/#table-of-contents","text":"Table of Contents Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install Contributing","title":"Table of Contents"},{"location":"ocp4-upi-powervs/#introduction","text":"The ocp4-upi-powervs project provides Terraform based automation code to help with the deployment of OpenShift Container Platform (OCP) 4.x on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . This project leverages the helpernode ansible playbook internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS).","title":"Introduction"},{"location":"ocp4-upi-powervs/#before-you-start","text":"The main branch must be used with latest OCP pre-release versions only. Ensure you checkout the correct release-VERSION branch depending on the version (4.5, 4.6 ...) of RedHat OpenShift you want to install: For example to use OCP 4.8 perform the following export VERSION=4.8 git clone --single-branch --branch release-$VERSION https://github.com/ocp-power-automation/ocp4-upi-powervs.git","title":"Before You Start"},{"location":"ocp4-upi-powervs/#make-it-better","text":"For bugs/enhancement requests etc. please open a GitHub issue","title":"Make It Better"},{"location":"ocp4-upi-powervs/#getting-started-with-powervs","text":"Power Systems Virtual Servers(IBM Cloud Docs) IBM Power Systems in the Multicloud(Youtube video) PowerVS (Youtube video)","title":"Getting Started With PowerVS"},{"location":"ocp4-upi-powervs/#automation-host-prerequisites","text":"The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems: - Mac OSX (Darwin) - Linux (x86_64/ppc64le) - Windows 10 Follow the guide to complete the prerequisites.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervs/#powervs-prerequisites","text":"Follow the guide to complete the PowerVS prerequisites.","title":"PowerVS Prerequisites"},{"location":"ocp4-upi-powervs/#ocp-install","text":"Follow the quickstart guide for OCP installation on PowerVS.","title":"OCP Install"},{"location":"ocp4-upi-powervs/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Attribution"},{"location":"ocp4-upi-powervs/CONTRIBUTING/","text":"Contributing This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project. Issues If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed. Pull Request Process To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s . Spec Formatting Conventions Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Contributing"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#contributing","text":"This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#issues","text":"If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed.","title":"Issues"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#pull-request-process","text":"To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Pull Request Process"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#spec-formatting-conventions","text":"Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Spec Formatting Conventions"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/","text":"Automation Host Prerequisites Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform PowerVS CLI Git Configure Your Firewall If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443 Automation Host Setup Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux/Windows. Terraform Terraform : Please open the link for downloading the latest Terraform. For validating the version run terraform version command after install. Terraform version 1.2.0 and above is required. Install Terraform and providers for Power environment: 1. Download and install the latest Terraform binary for Linux/ppc64le from https://github.com/ppc64le-development/terraform-ppc64le/releases. 2. Download the required Terraform providers for Power into your TF project directory: $ cd <path_to_TF_project> $ mkdir -p ./providers $ curl -fsSL https://github.com/ocp-power-automation/terraform-providers-power/releases/download/v0.11/archive.zip -o archive.zip $ unzip -o ./archive.zip -d ./providers $ rm -f ./archive.zip Initialize Terraform at your TF project directory: $ terraform init --plugin-dir ./providers PowerVS CLI PowerVS CLI : Please download and install the CLI by referring to the following instructions . Alternatively, you can use IBM Cloud shell directly from the browser itself. Git Git : Please refer to the link for instructions on installing Git.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#automation-host-prerequisites","text":"Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform PowerVS CLI Git","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#configure-your-firewall","text":"If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443","title":"Configure Your Firewall"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#automation-host-setup","text":"Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux/Windows.","title":"Automation Host Setup"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#terraform","text":"Terraform : Please open the link for downloading the latest Terraform. For validating the version run terraform version command after install. Terraform version 1.2.0 and above is required. Install Terraform and providers for Power environment: 1. Download and install the latest Terraform binary for Linux/ppc64le from https://github.com/ppc64le-development/terraform-ppc64le/releases. 2. Download the required Terraform providers for Power into your TF project directory: $ cd <path_to_TF_project> $ mkdir -p ./providers $ curl -fsSL https://github.com/ocp-power-automation/terraform-providers-power/releases/download/v0.11/archive.zip -o archive.zip $ unzip -o ./archive.zip -d ./providers $ rm -f ./archive.zip Initialize Terraform at your TF project directory: $ terraform init --plugin-dir ./providers","title":"Terraform"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#powervs-cli","text":"PowerVS CLI : Please download and install the CLI by referring to the following instructions . Alternatively, you can use IBM Cloud shell directly from the browser itself.","title":"PowerVS CLI"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#git","text":"Git : Please refer to the link for instructions on installing Git.","title":"Git"},{"location":"ocp4-upi-powervs/docs/known_issues/","text":"Known Issues This page lists the known issues and potential next steps when deploying OpenShift (OCP) in Power Systems Virtual Server (PowerVS) Terraform apply returns the following error Error : timeout - last error: Error connecting to bastion: dial tcp 161.156.139.82:22: connect: operation timed out Cause : The public network attached to bastion is not reachable. Ping to the public/external IP of bastion node (eg. 161.156.139.82) will not return any response Workaround : Re-run TF again and if it doesn't help, destroy the TF resources and re-run. If it doesn't work, then please open a support case with IBM Cloud to fix issue with reachability of public IP for PowerVS instance. RHCOS instances in dashboard shows \\\"Warning\\\" Status Cause : This is due to RSCT daemon not being available for RHCOS Workaround : None You can ignore this. This will be fixed soon. Terraform apply fails with instance is not reachable error Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows grub rescue Workaround : Rebooting the instance helps. If still facing the same issue, try destroying the TF resources and re-running the deployment Terraform apply fails with instance is not reachable error Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows connect: network is unreachable Cause : Unknown Workaround : Rebooting the instance helps. Terraform provisioning fails with the following error Error : Failed to get the instance Get https://eu-de.power-iaas.cloud.ibm.com/pcloud/v1/cloud-instances/d239797b-7b2e- 4790-a29d-439567556c83/pvm-instances/7d836f7d-8f21-4bef-9e10-ae6d8c2167a1: context deadline exceeded on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Reapply will also fail **module.nodes.ibm_pi_instance.master[0]: Creating... Error: Failed to provision {\"description\":\"bad request: invalid name server name already exists for cloud- instance\",\"error\":\"bad request\"} on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Cause : IBM Cloud API failed but resource got created and TF doesn't know that the resource got created. Workaround : Manually delete the resource from the dashboard and re-apply Terraform apply fails with these errors Error : module.install.null_resource.install (remote-exec): information. module.install.null_resource.install (remote-exec): changed: [192.168.25.12] => {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/libexec/platform-python\"}, \"changed\": true, \"cmd\": \"if lsmod|grep -q 'ibmveth'; then\\n sudo sysctl -w net.ipv4.route.min_pmtu=1450;\\n sudo sysctl -w net.ipv4.ip_no_pmtu_disc=1;\\n echo 'net.ipv4.route.min_pmtu = 1450' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\n echo 'net.ipv4.ip_no_pmtu_disc = 1' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\nfi\\n\", \"delta\": \"0:00:00.078644\", \"end\": \"2020-09-18 16:25:46.414601\", \"rc\": 0, \"start\": \"2020-09-18 16:25:46.335957\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"net.ipv4.route.min_pmtu = 1450\\nnet.ipv4.ip_no_pmtu_disc = 1\", \"stdout_lines\": [\"net.ipv4.route.min_pmtu = 1450\", \"net.ipv4.ip_no_pmtu_disc = 1\"]} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.105 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.12 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.121 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.15 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.235 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.39 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.5 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.82 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 module.install.null_resource.config (remote-exec): TASK [Downloading OCP4 Installer] * * * * * * **** > module.install.null_resource.config: Still creating... [1m40s elapsed] > module.install.null_resource.config (remote-exec): fatal: [localhost]: FAILED! => {\"changed\": false, \"dest\": \"/usr/local/src/openshift-install-linux.tar.gz\", \"elapsed\": 10, \"msg\": \"Request failed: \", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/4.5.4/openshift-install-linux.tar.gz\"} module.install.null_resource.install (remote-exec): fatal: [192.168.25.167]: UNREACHABLE! => {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 192.168.25.167 port 22: Connection timed out\", \"unreachable\": true} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.107 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.165 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.167 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.212 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.64 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.76 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.78 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.81 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Workaround : Re-run the terraform apply command.","title":"Known Issues"},{"location":"ocp4-upi-powervs/docs/known_issues/#known-issues","text":"This page lists the known issues and potential next steps when deploying OpenShift (OCP) in Power Systems Virtual Server (PowerVS)","title":"Known Issues"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-returns-the-following-error","text":"Error : timeout - last error: Error connecting to bastion: dial tcp 161.156.139.82:22: connect: operation timed out Cause : The public network attached to bastion is not reachable. Ping to the public/external IP of bastion node (eg. 161.156.139.82) will not return any response Workaround : Re-run TF again and if it doesn't help, destroy the TF resources and re-run. If it doesn't work, then please open a support case with IBM Cloud to fix issue with reachability of public IP for PowerVS instance.","title":"Terraform apply returns the following error"},{"location":"ocp4-upi-powervs/docs/known_issues/#rhcos-instances-in-dashboard-shows-warning-status","text":"Cause : This is due to RSCT daemon not being available for RHCOS Workaround : None You can ignore this. This will be fixed soon.","title":"RHCOS instances in dashboard shows \\\"Warning\\\" Status"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-fails-with-instance-is-not-reachable-error","text":"Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows grub rescue Workaround : Rebooting the instance helps. If still facing the same issue, try destroying the TF resources and re-running the deployment","title":"Terraform apply fails with instance is not reachable error"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-fails-with-instance-is-not-reachable-error_1","text":"Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows connect: network is unreachable Cause : Unknown Workaround : Rebooting the instance helps.","title":"Terraform apply fails with instance is not reachable error"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-provisioning-fails-with-the-following-error","text":"Error : Failed to get the instance Get https://eu-de.power-iaas.cloud.ibm.com/pcloud/v1/cloud-instances/d239797b-7b2e- 4790-a29d-439567556c83/pvm-instances/7d836f7d-8f21-4bef-9e10-ae6d8c2167a1: context deadline exceeded on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Reapply will also fail **module.nodes.ibm_pi_instance.master[0]: Creating... Error: Failed to provision {\"description\":\"bad request: invalid name server name already exists for cloud- instance\",\"error\":\"bad request\"} on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Cause : IBM Cloud API failed but resource got created and TF doesn't know that the resource got created. Workaround : Manually delete the resource from the dashboard and re-apply","title":"Terraform provisioning fails with the following error"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-fails-with-these-errors","text":"Error : module.install.null_resource.install (remote-exec): information. module.install.null_resource.install (remote-exec): changed: [192.168.25.12] => {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/libexec/platform-python\"}, \"changed\": true, \"cmd\": \"if lsmod|grep -q 'ibmveth'; then\\n sudo sysctl -w net.ipv4.route.min_pmtu=1450;\\n sudo sysctl -w net.ipv4.ip_no_pmtu_disc=1;\\n echo 'net.ipv4.route.min_pmtu = 1450' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\n echo 'net.ipv4.ip_no_pmtu_disc = 1' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\nfi\\n\", \"delta\": \"0:00:00.078644\", \"end\": \"2020-09-18 16:25:46.414601\", \"rc\": 0, \"start\": \"2020-09-18 16:25:46.335957\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"net.ipv4.route.min_pmtu = 1450\\nnet.ipv4.ip_no_pmtu_disc = 1\", \"stdout_lines\": [\"net.ipv4.route.min_pmtu = 1450\", \"net.ipv4.ip_no_pmtu_disc = 1\"]} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.105 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.12 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.121 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.15 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.235 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.39 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.5 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.82 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 module.install.null_resource.config (remote-exec): TASK [Downloading OCP4 Installer] * * * * * * **** > module.install.null_resource.config: Still creating... [1m40s elapsed] > module.install.null_resource.config (remote-exec): fatal: [localhost]: FAILED! => {\"changed\": false, \"dest\": \"/usr/local/src/openshift-install-linux.tar.gz\", \"elapsed\": 10, \"msg\": \"Request failed: \", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/4.5.4/openshift-install-linux.tar.gz\"} module.install.null_resource.install (remote-exec): fatal: [192.168.25.167]: UNREACHABLE! => {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 192.168.25.167 port 22: Connection timed out\", \"unreachable\": true} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.107 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.165 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.167 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.212 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.64 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.76 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.78 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.81 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Workaround : Re-run the terraform apply command.","title":"Terraform apply fails with these errors"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/","text":"PowerVS Prerequisites Create an IBM Cloud account. If you don\u2019t already have one, you need a paid IBM Cloud account to create your Power Systems Virtual Server instance. To create an account, go to: cloud.ibm.com . Create an IBM Cloud account API key Please refer to the following documentation to create an API key. Create Power Systems Virtual Server Service Instance After you have an active IBM Cloud account, you can create a Power Systems Virtual Server service. To do so, perform the following steps: Log in to the IBM Cloud dashboard and search for Power in the catalog. Select Power Systems Virtual Server Fill required details Provide a meaningful name for your instance in the Service name field and select the proper resource group . More details on resource groups is available from the following link Create Service Click on \" Create \" to create the service instance. Create Private Network A private network is required for your OpenShift cluster. Perform the following steps to create a private network for the Power Systems Virtual Server service instance created in the previous step. Select the previously created \" Service Instance \" and create a private subnet by clicking \" Subnets \" and providing the required inputs. Note: If you see a screen displaying CRN and GUID, then click \"View full details\" to access the \"Subnet\" creation page. Provide the network details and click \"Create subnet\" On successful network creation, the following output will be displayed in the dashboard. Enable communication over the private network Two options are available to enable communication over the private network. Option 1 You can use the IBM Cloud CLI with the latest power-iaas plug-in (version 0.3.4 or later) to enable a private network communication. Refer: https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-managing-cloud-connections This requires attaching the private network to an IBM Cloud Direct Link Connect 2.0 connection. Perform the following steps to enable private network communication by attaching to the Direct Link Connect 2.0 connection. Select a specific service instance You\u2019ll need the CRN of the service instance created earlier (for example, ocp-powervs-test-1). ibmcloud pi service-target crn:v1:bluemix:public:power-iaas:tok04:a/65b64c1f1c29460e8c2e4bbfbd893c2c:e4bb3d9d-a37c-4b1f-a923-4537c0c8beb3:: Get the ID of the private network ibmcloud pi nets | grep -w ocp-net ID 93cc386a-53c5-4aef-9882-4294025c5e1f Name ocp-net Type vlan VLAN 413 CIDR Block 192.168.201.0/24 IP Range [192.168.201.2 192.168.201.254] Gateway 192.168.201.1 DNS 127.0.0.1 You\u2019ll need the ID in subsequent steps. Get the Direct Link Connect connection ID ibmcloud pi cons ID Name Link Status Speed 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e ocp-powervs-dl idle 10000 Get the ID of the connection. If you don\u2019t have an existing Direct Link Connect 2.0 connection provisioned under your account, then you can create a new connection using the IBM Cloud CLI. A highly available Direct Link Connect 2.0 connection between the Power Virtual Server and IBM Cloud comes free of cost. Refer: https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-ordering-direct-link-connect ibmcloud pi conc ocp-powervs-dl --speed 10000 Attach the private network to Direct Link Connect 2.0 (connection) ibmcloud pi conan 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e --network e1b90247-a504-4468-8662-8f785533067d This can take 3 to 5 minutes to become active. Verify the status of the attachment ibmcloud pi con 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e ID 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e Name ocp-powervs-dl Link Status idle Speed 10000 Creation Date 2021-05-13T13:17:08.093Z Global Routing false IBM IPAddress 169.254.0.1/30 User IPAddress 169.254.0.2/30 Metered false Classic false Networks ID: e1b90247-a504-4468-8662-8f785533067d Name: ocp-net VlanID: 392 The output shows that the ocp-net private network is attached to Direct Link Connect 2.0. This enables inter VM communication on the private network as well as communication with IBM Cloud over Direct Link. Option 2 If you don\u2019t want to use Direct Link, then you\u2019ll need to raise a service request to enable private network communication. Perform the following steps to raise the service request. Click on Support in the top bar of the dashboard and scroll down to Contact Support , then select \" Create a case \" Select \" Power Systems Virtual Server \" tile Complete the details as shown using the following template: - [Subject:] Enable communication between PowerVS instances on private network - [Body:] Please enable IP communication between PowerVS instances for the following private network: Name: <your-subnet-name-from-above> Type: Private CIDR: <your ip subnet-from-above> VLAN ID: <your-vlan-id> (listed in your subnet details post-creation) Location: <your-location> (listed in your subnet details post-creation) Service Instance: <your-service-name> Following is a complete example of the support case content. Please enable IP communication between PowerVS instances for the following private network: Name: ocp-net Type: Private CIDR: 192.168.25.0/24 VLAN ID: 293 Location: eu-de-2 Service Instance: ocp-powervs-frankfurt-2 Click \" Continue \" to accept agreements, and then Click \" Submit case \". This usually takes a day to get enabled. RHCOS and RHEL/CentOS 8.X Images for OpenShift RHEL image is used for bastion and RHCOS is used for the OpenShift cluster nodes. You'll need to create OVA formatted images for RHEL and RHCOS, upload them to IBM Cloud Object storage and then import these images as boot images in your PowerVS service instance. Further, the image disk should be minimum of 120 GB in size. Creating OVA images If you have PowerVC then you can follow the instructions provided in the link to export an existing PowerVC image to OVA image. You can also use the following tool to convert Qcow2 image to OVA. Qcow2 Image Links RHEL 8.3 Qcow2 image is available from the following link CentOS 8.3 Wcow2 image is available from the following link RHCOS Qcow2 image is available from the following link Note: RHCOS image version is tied to the specific OCP release. For example RHCOS-4.6 image needs to be used for OCP 4.6 release. Uploading to IBM Cloud Object Storage Create IBM Cloud Object Storage service and bucket Please refer to the following link for instructions to create IBM Cloud Object Storage service and required storage bucket to upload the OVA images. Create secret and access keys with Hash-based Message Authentication Code (HMAC) Please refer to the following link for instructions to create the keys required for importing the images into your PowerVS service instance. Upload the OVA image to Cloud Object storage bucket Please refer to the following link for uploading the OVA image to the respective bucket. Alternatively you can also use the following tool . Importing the images in PowerVS Choose the previously created PowerVS \"Service Instance\" , click \"View full details\" and select \"Boot images\" . Click the \"Import image\" option and fill the requisite details like image name, storage type and cloud object storage details. Example screenshot showing import of RHEL image that is used for bastion Example screenshot showing import of RHCOS image used for OCP Your PowerVS service instance is now ready for OpenShift clusters.","title":"**PowerVS Prerequisites**"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#powervs-prerequisites","text":"","title":"PowerVS Prerequisites"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#create-an-ibm-cloud-account","text":"If you don\u2019t already have one, you need a paid IBM Cloud account to create your Power Systems Virtual Server instance. To create an account, go to: cloud.ibm.com .","title":"Create an IBM Cloud account."},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#create-an-ibm-cloud-account-api-key","text":"Please refer to the following documentation to create an API key.","title":"Create an IBM Cloud account API key"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#create-power-systems-virtual-server-service-instance","text":"After you have an active IBM Cloud account, you can create a Power Systems Virtual Server service. To do so, perform the following steps: Log in to the IBM Cloud dashboard and search for Power in the catalog. Select Power Systems Virtual Server Fill required details Provide a meaningful name for your instance in the Service name field and select the proper resource group . More details on resource groups is available from the following link Create Service Click on \" Create \" to create the service instance.","title":"Create Power Systems Virtual Server Service Instance"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#create-private-network","text":"A private network is required for your OpenShift cluster. Perform the following steps to create a private network for the Power Systems Virtual Server service instance created in the previous step. Select the previously created \" Service Instance \" and create a private subnet by clicking \" Subnets \" and providing the required inputs. Note: If you see a screen displaying CRN and GUID, then click \"View full details\" to access the \"Subnet\" creation page. Provide the network details and click \"Create subnet\" On successful network creation, the following output will be displayed in the dashboard.","title":"Create Private Network"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#enable-communication-over-the-private-network","text":"Two options are available to enable communication over the private network. Option 1 You can use the IBM Cloud CLI with the latest power-iaas plug-in (version 0.3.4 or later) to enable a private network communication. Refer: https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-managing-cloud-connections This requires attaching the private network to an IBM Cloud Direct Link Connect 2.0 connection. Perform the following steps to enable private network communication by attaching to the Direct Link Connect 2.0 connection. Select a specific service instance You\u2019ll need the CRN of the service instance created earlier (for example, ocp-powervs-test-1). ibmcloud pi service-target crn:v1:bluemix:public:power-iaas:tok04:a/65b64c1f1c29460e8c2e4bbfbd893c2c:e4bb3d9d-a37c-4b1f-a923-4537c0c8beb3:: Get the ID of the private network ibmcloud pi nets | grep -w ocp-net ID 93cc386a-53c5-4aef-9882-4294025c5e1f Name ocp-net Type vlan VLAN 413 CIDR Block 192.168.201.0/24 IP Range [192.168.201.2 192.168.201.254] Gateway 192.168.201.1 DNS 127.0.0.1 You\u2019ll need the ID in subsequent steps. Get the Direct Link Connect connection ID ibmcloud pi cons ID Name Link Status Speed 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e ocp-powervs-dl idle 10000 Get the ID of the connection. If you don\u2019t have an existing Direct Link Connect 2.0 connection provisioned under your account, then you can create a new connection using the IBM Cloud CLI. A highly available Direct Link Connect 2.0 connection between the Power Virtual Server and IBM Cloud comes free of cost. Refer: https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-ordering-direct-link-connect ibmcloud pi conc ocp-powervs-dl --speed 10000 Attach the private network to Direct Link Connect 2.0 (connection) ibmcloud pi conan 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e --network e1b90247-a504-4468-8662-8f785533067d This can take 3 to 5 minutes to become active. Verify the status of the attachment ibmcloud pi con 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e ID 89fcfd7c-ec74-473b-ba09-4cd95fa47e2e Name ocp-powervs-dl Link Status idle Speed 10000 Creation Date 2021-05-13T13:17:08.093Z Global Routing false IBM IPAddress 169.254.0.1/30 User IPAddress 169.254.0.2/30 Metered false Classic false Networks ID: e1b90247-a504-4468-8662-8f785533067d Name: ocp-net VlanID: 392 The output shows that the ocp-net private network is attached to Direct Link Connect 2.0. This enables inter VM communication on the private network as well as communication with IBM Cloud over Direct Link. Option 2 If you don\u2019t want to use Direct Link, then you\u2019ll need to raise a service request to enable private network communication. Perform the following steps to raise the service request. Click on Support in the top bar of the dashboard and scroll down to Contact Support , then select \" Create a case \" Select \" Power Systems Virtual Server \" tile Complete the details as shown using the following template: - [Subject:] Enable communication between PowerVS instances on private network - [Body:] Please enable IP communication between PowerVS instances for the following private network: Name: <your-subnet-name-from-above> Type: Private CIDR: <your ip subnet-from-above> VLAN ID: <your-vlan-id> (listed in your subnet details post-creation) Location: <your-location> (listed in your subnet details post-creation) Service Instance: <your-service-name> Following is a complete example of the support case content. Please enable IP communication between PowerVS instances for the following private network: Name: ocp-net Type: Private CIDR: 192.168.25.0/24 VLAN ID: 293 Location: eu-de-2 Service Instance: ocp-powervs-frankfurt-2 Click \" Continue \" to accept agreements, and then Click \" Submit case \". This usually takes a day to get enabled.","title":"Enable communication over the private network"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#rhcos-and-rhelcentos-8x-images-for-openshift","text":"RHEL image is used for bastion and RHCOS is used for the OpenShift cluster nodes. You'll need to create OVA formatted images for RHEL and RHCOS, upload them to IBM Cloud Object storage and then import these images as boot images in your PowerVS service instance. Further, the image disk should be minimum of 120 GB in size.","title":"RHCOS and RHEL/CentOS 8.X Images for OpenShift"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#creating-ova-images","text":"If you have PowerVC then you can follow the instructions provided in the link to export an existing PowerVC image to OVA image. You can also use the following tool to convert Qcow2 image to OVA. Qcow2 Image Links RHEL 8.3 Qcow2 image is available from the following link CentOS 8.3 Wcow2 image is available from the following link RHCOS Qcow2 image is available from the following link Note: RHCOS image version is tied to the specific OCP release. For example RHCOS-4.6 image needs to be used for OCP 4.6 release.","title":"Creating OVA images"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#uploading-to-ibm-cloud-object-storage","text":"Create IBM Cloud Object Storage service and bucket Please refer to the following link for instructions to create IBM Cloud Object Storage service and required storage bucket to upload the OVA images. Create secret and access keys with Hash-based Message Authentication Code (HMAC) Please refer to the following link for instructions to create the keys required for importing the images into your PowerVS service instance. Upload the OVA image to Cloud Object storage bucket Please refer to the following link for uploading the OVA image to the respective bucket. Alternatively you can also use the following tool .","title":"Uploading to IBM Cloud Object Storage"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#importing-the-images-in-powervs","text":"Choose the previously created PowerVS \"Service Instance\" , click \"View full details\" and select \"Boot images\" . Click the \"Import image\" option and fill the requisite details like image name, storage type and cloud object storage details. Example screenshot showing import of RHEL image that is used for bastion Example screenshot showing import of RHCOS image used for OCP Your PowerVS service instance is now ready for OpenShift clusters.","title":"Importing the images in PowerVS"},{"location":"ocp4-upi-powervs/docs/quickstart/","text":"Installation Quickstart Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up Download the Automation Code You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervs.git $ cd ocp4-upi-powervs All further instructions assumes you are in the code directory eg. ocp4-upi-powervs Setup Terraform Variables Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export IBMCLOUD_API_KEY=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history Start Install Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 If using template flavors for nodes configuration, then do the following. $ terraform init $ terraform apply -var-file var.tfvars -var-file compute-vars/<flavor-type>.tfvars Here, is the node configuration template name. By default, medium flavor type is specified. If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 -var ibmcloud_api_key=\"$IBMCLOUD_API_KEY\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Note : We have used parallelism to restrict parallel instance creation requests using the PowerVS client. This is due to a known issue where the apply fails at random parallel instance create requests. If you still get the error while creating the instance, you will have to delete the failed instance from PowerVS console and then run the apply command again. Now wait for the installation to complete. It may take around 60 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds. Post Install Delete Bootstrap Node Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {memory = \"16\", processors = \"0.5\", \"count\" = 0} Run command terraform apply -var-file var.tfvars Create API and Ingress DNS Records Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>.<cluster_domain>. IN A <bastion_address> *.apps.<cluster_id>.<cluster_domain>. IN A <bastion_address> You\u2019ll need dns_entries . This is printed at the end of a successful install. Alternatively, you can retrieve it anytime by running terraform output dns_entries from the install directory. An example dns_entries output: api.test-cluster-9a4f.mydomain.com. IN A 16.20.34.5 *.apps.test-cluster-9a4f.mydomain.com. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_address> api.<cluster_id>.<cluster_domain> console-openshift-console.apps.<cluster_id>.<cluster_domain> integrated-oauth-server-openshift-authentication.apps.<cluster_id>.<cluster_domain> oauth-openshift.apps.<cluster_id>.<cluster_domain> prometheus-k8s-openshift-monitoring.apps.<cluster_id>.<cluster_domain> grafana-openshift-monitoring.apps.<cluster_id>.<cluster_domain> <app name>.apps.<cluster_id>.<cluster_domain> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output etc_hosts_entries from the install directory. As an example, for the following etc_hosts_entries 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ``` Cluster Access OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* . Using CLI OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output oc_server_url from the install directory. https://test-cluster-9a4f.mydomain.com:6443 In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 11h v1.19.0+43983cd master-1 Ready master 11h v1.19.0+43983cd master-2 Ready master 11h v1.19.0+43983cd worker-0 Ready worker 11h v1.19.0+43983cd worker-1 Ready worker 11h v1.19.0+43983cd Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config . Using Web UI The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output web_console_url from the install directory. https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file. Clean up To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars -parallelism=3 to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Installation Quickstart"},{"location":"ocp4-upi-powervs/docs/quickstart/#installation-quickstart","text":"Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up","title":"Installation Quickstart"},{"location":"ocp4-upi-powervs/docs/quickstart/#download-the-automation-code","text":"You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervs.git $ cd ocp4-upi-powervs All further instructions assumes you are in the code directory eg. ocp4-upi-powervs","title":"Download the Automation Code"},{"location":"ocp4-upi-powervs/docs/quickstart/#setup-terraform-variables","text":"Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export IBMCLOUD_API_KEY=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history","title":"Setup Terraform Variables"},{"location":"ocp4-upi-powervs/docs/quickstart/#start-install","text":"Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 If using template flavors for nodes configuration, then do the following. $ terraform init $ terraform apply -var-file var.tfvars -var-file compute-vars/<flavor-type>.tfvars Here, is the node configuration template name. By default, medium flavor type is specified. If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 -var ibmcloud_api_key=\"$IBMCLOUD_API_KEY\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Note : We have used parallelism to restrict parallel instance creation requests using the PowerVS client. This is due to a known issue where the apply fails at random parallel instance create requests. If you still get the error while creating the instance, you will have to delete the failed instance from PowerVS console and then run the apply command again. Now wait for the installation to complete. It may take around 60 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds.","title":"Start Install"},{"location":"ocp4-upi-powervs/docs/quickstart/#post-install","text":"","title":"Post Install"},{"location":"ocp4-upi-powervs/docs/quickstart/#delete-bootstrap-node","text":"Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {memory = \"16\", processors = \"0.5\", \"count\" = 0} Run command terraform apply -var-file var.tfvars","title":"Delete Bootstrap Node"},{"location":"ocp4-upi-powervs/docs/quickstart/#create-api-and-ingress-dns-records","text":"Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>.<cluster_domain>. IN A <bastion_address> *.apps.<cluster_id>.<cluster_domain>. IN A <bastion_address> You\u2019ll need dns_entries . This is printed at the end of a successful install. Alternatively, you can retrieve it anytime by running terraform output dns_entries from the install directory. An example dns_entries output: api.test-cluster-9a4f.mydomain.com. IN A 16.20.34.5 *.apps.test-cluster-9a4f.mydomain.com. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_address> api.<cluster_id>.<cluster_domain> console-openshift-console.apps.<cluster_id>.<cluster_domain> integrated-oauth-server-openshift-authentication.apps.<cluster_id>.<cluster_domain> oauth-openshift.apps.<cluster_id>.<cluster_domain> prometheus-k8s-openshift-monitoring.apps.<cluster_id>.<cluster_domain> grafana-openshift-monitoring.apps.<cluster_id>.<cluster_domain> <app name>.apps.<cluster_id>.<cluster_domain> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output etc_hosts_entries from the install directory. As an example, for the following etc_hosts_entries 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ```","title":"Create API and Ingress DNS Records"},{"location":"ocp4-upi-powervs/docs/quickstart/#cluster-access","text":"OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* .","title":"Cluster Access"},{"location":"ocp4-upi-powervs/docs/quickstart/#using-cli","text":"OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output oc_server_url from the install directory. https://test-cluster-9a4f.mydomain.com:6443 In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 11h v1.19.0+43983cd master-1 Ready master 11h v1.19.0+43983cd master-2 Ready master 11h v1.19.0+43983cd worker-0 Ready worker 11h v1.19.0+43983cd worker-1 Ready worker 11h v1.19.0+43983cd Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config .","title":"Using CLI"},{"location":"ocp4-upi-powervs/docs/quickstart/#using-web-ui","text":"The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output web_console_url from the install directory. https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file.","title":"Using Web UI"},{"location":"ocp4-upi-powervs/docs/quickstart/#clean-up","text":"To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars -parallelism=3 to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Clean up"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/","text":"How to use var.tfvars How to use var.tfvars Introduction IBM Cloud Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations Introduction This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf IBM Cloud Details These set of variables specify the access key and PowerVS location details. ibmcloud_api_key = \"xyzaaaaaaaabcdeaaaaaa\" ibmcloud_region = \"xya\" ibmcloud_zone = \"abc\" service_instance_id = \"abc123xyzaaaa\" You'll need to create an API key to use the automation code. Please refer to the following instructions to generate API key - https://cloud.ibm.com/docs/account?topic=account-userapikey In order to retrieve the PowerVS region, zone and instance specific details please use the IBM Cloud CLI. Run ibmcloud pi service-list . It will list the service instance names with IDs. The ID will be of the form crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6:: The 6th field is the ibmcloud_zone and 8th field is the service_instance_id $ echo \"crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6::\" | cut -f6,8 -d\":\" eu-de-1:360a5df8-3f00-44b2-bd9f-d9a51fe53de6 Following are the region and zone mapping: ibmcloud_region ibmcloud_zone eu-de eu-de-1 eu-de eu-de-2 dal dal12 lon lon04 lon lon06 syd syd04 sao sao01 tor tor01 tok tok04 us-east us-east NOTE: us-east is Washington, DC datacenter. Tieing all these, the values to be used will be as shown below: ibmcloud_region = eu-de ibmcloud_zone = eu-de-1 service_instance_id = 360a5df8-3f00-44b2-bd9f-d9a51fe53de6 OpenShift Cluster Details These set of variables specify the cluster capacity. Change the values as per your requirement. The defaults (recommended config) should suffice for most of the common use-cases. bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} You can also choose one of the default node configuration templates that are stored in the compute-vars directory, as per your requirements. The default flavors present under the compute-vars folder: small.tfvars medium.tfvars large.tfvars memory is in GBs and count specifies the number of VMs that should be created for each type. To enable high availability (HA) for cluster services running on the bastion set the bastion count value to 2. Note that in case of HA, the automation will not setup NFS storage. count of 1 for bastion implies the default non-HA bastion setup. You can optionally set the worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. To attach additional volumes to master or worker nodes, set the optional data_volume_count key to the number of volumes that is to be attached and the data_volume_size to the size (in GB) for each volume. master = {memory = \"32\", processors = \"0.5\", \"count\" = 3, data_volume_count = 0, data_volume_size = 100} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2, data_volume_count = 0, data_volume_size = 100} For PowerVS processors are equal to entitled physical count. So N processors == N physical core entitlements == ceil[N] vCPUs. Here are some examples to help you understand the relationship. Example 1 0.5 processors == 0.5 physical core entitlements == ceil[0.5] = 1 vCPU == 8 logical OS CPUs (SMT=8) Example 2 1.5 processors == 1.5 physical core entitlements == ceil[1.5] = 2 vCPU == 16 logical OS CPUs (SMT=8) Example 3 2 processors == 2 physical core entitlements == ceil[2] = 2 vCPU == 16 logical OS CPUs (SMT=8) These set of variables specify the RHEL and RHCOS boot image names. These images should have been already imported in your PowerVS service instance. Change the image names according to your environment. Ensure that you use the correct RHCOS image specific to the pre-release version rhel_image_name = \"<rhel_or_centos_image-name>\" rhcos_image_name = \"<rhcos-image-name>\" Note that the boot images should have a minimum disk size of 120GB These set of variables should be provided when RHCOS image should be imported from public bucket of cloud object storage to your PowerVS service instance rhcos_import_image = true # true/false (default=false) rhcos_import_image_filename = \"rhcos-411-85-202203181612-0-ppc64le-powervs.ova.gz\" # RHCOS boot image file name available in cloud object storage rhcos_import_image_storage_type = \"tier1\" # tier1/tier3 (default=tier1) Storage type in PowerVS where image needs to be uploaded This variable specifies the name of the private network that is configured in your PowerVS service instance. network_name = \"ocp-net\" These set of variables specify the type of processor and physical system type to be used for the VMs. Change the default values according to your requirement. processor_type = \"shared\" #Can be shared or dedicated system_type = \"s922\" #Can be either s922 or e980 These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" #Set it to an appropriate username for non-root user access public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" rhel_username is set to root. rhel_username can be set to an appropriate username having superuser privileges with no password prompt. Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details, RHEL subscription supports two methods: one is using username and password, the other is using activation key. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . If you are using CentOS as the bastion image, then leave these variables as-is. rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" Or define following variables to use activation key for RHEL subscription: rhel_subscription_org = \"org-id\" rhel_subscription_activationkey = \"activation-key\" This variable specifies the number of hardware threads (SMT) that's used for the bastion node. Default setting should be fine for majority of the use-cases. rhel_smt = 4 OpenShift Installation Details These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" use_zone_info_for_names = true Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp If cluster_id_prefix is not set, the cluster_id will be used only without prefix. A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 32 characters. The use_zone_info_for_names is a flag to indicate whether to use cluster_id - ibmcloud_zone or only cluster_id as name prefix for resource naming on PowerVS. The default value is set to true to use zone info in names, and the total length of cluster_id_prefix - cluster_id - ibmcloud_zone should not exceed 32 characters. FIPS Variable for OpenShift deployment These variables will be used for deploying OCP in FIPS mode. Change the values as per your requirement. fips_compliant = false Note: Once fips_compliant set to true it will enable FIPS on the OCP cluster and also on bastion nodes. At the end of install the bastion nodes will be rebooted. Using IBM Cloud Services You can use IBM Cloud classic DNS and VPC Load Balancer services for running the OCP cluster. When this feature is enabled the services called named (DNS) and haproxy (Load Balancer) will not be running on the bastion/helpernode. Ensure you have setup DirectLink with IBM Cloud VPC over the private network in cloud instance. Also, ensure you have registered a DNS domain and use it as given in cluster_domain variable. IMPORTANT : This is an experimental feature at present. Please manually set variables setup_snat = true and setup_squid_proxy = false for using IBM Cloud services. This will allow the cluster nodes have public internet access without a proxy server. Below variables needs to be set in order to use the IBM Cloud services. use_ibm_cloud_services = true ibm_cloud_vpc_name = \"ocp-vpc\" ibm_cloud_vpc_subnet_name = \"ocp-subnet\" These set of variables specify the username and API key for accessing IBM Cloud services. The default combination should suffice for most of the common use-cases. iaas_classic_username = \"apikey\" iaas_classic_api_key = \"\" # if empty, will default to ibmcloud_api_key. iaas_vpc_region = \"\" # if empty, will default to ibmcloud_region. Note: iaas_classic_username , iaas_classic_api_key and iaas_vpc_region variables are optional, These variables need to be set only when using a different classic username, key and vpc region. By default apikey will be used as the iaas_class_username , ibmcloud_api_key will be used as the iaas_classic_api_key and ibmcloud_region will be used as the iaas_vpc_region . Note that non-default values for these variables can also be passed via environment variables IAAS_CLASSIC_USERNAME and IAAS_CLASSIC_API_KEY respectively. Misc Customizations These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/redhat-cop/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" This variable specify if bastion should poll for the Health Status to be OK or WARNING. Default is OK. bastion_health_status = \"OK\" This variable specify the MTU value for the private network interface on RHEL and RHCOS nodes. The CNI network will have - 50 for OpenshiftSDN and - 100 for OVNKubernetes network provider. private_network_mtu = 1450 These variables can be used when debugging ansible playbooks. installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable can be used to change the repository name for installing ansible package on RHEL. ansible_repo_name = \"ansible-2.9-for-rhel-8-ppc64le-rpms\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of day-1 kernel arguments for the cluster nodes. To add kernel arguments to master or worker nodes, using MachineConfig object and inject that object into the set of manifest files used by Ignition during cluster setup. rhcos_pre_kernel_options = [] Example 1 rhcos_pre_kernel_options = [\"rd.multipath=default\",\"root=/dev/disk/by-label/dm-mpath-root\"] List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] This is a Map of Node labels and its values. Some of the well known labels such as topology.kubernetes.io/region, topology.kubernetes.io/zone and node.kubernetes.io/instance-type are automated. More custom labels can be added using the node_labels map variable. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. node_labels = {} Example 1 node_labels = {\"failure-domain.beta.kubernetes.io/region\": \"mon\",\"failure-domain.beta.kubernetes.io/zone\": \"mon01\"} These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = true If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_shareable = false The following variables are specific to upgrading an existing installation. upgrade_image = \"\" #(e.g. `\"quay.io/openshift-release-dev/ocp-release-nightly@sha256:xxxxx\"`) upgrade_version = \"\" upgrade_pause_time = \"70\" upgrade_delay_time = \"600\" One of the two varaibles upgrade_image or upgrade_version is required for upgrading the cluster. upgrade_image having higher precedence than upgrade_version . The following variables are specific to performing EUS upgrades. eus_upgrade_version = \"4.11.14\" eus_upgrade_channel = \"stable-4.11\" #(stable-4.x, fast-4.x, candidate-4.x, eus-4.x) eus_upgrade_image = \"quay.io/openshift-release-dev/ocp-release:4.11.14-ppc64le\" eus_upstream = \"\" (e.g. `\"https://ppc64le.ocp.releases.ci.openshift.org/graph\"`) The following variables are specific to enable the connectivity between OCP nodes in PowerVS and IBM Cloud infrastructure over DirectLink. ibm_cloud_dl_endpoint_net_cidr = \"\" ibm_cloud_http_proxy = \"\" This variable is used to set the default Container Network Interface (CNI) network provider such as OpenShiftSDN or OVNKubernetes cni_network_provider = \"OVNKubernetes\" This variable is used to enable SNAT for OCP nodes. When using SNAT, the OCP nodes will be able to access public internet without using a proxy setup_snat = true These set of variables are specific for CSI Driver configuration and installation. csi_driver_install = false csi_driver_type = \"stable\" csi_driver_version = \"v0.1.1\" IMPORTANT : This is an experimental feature and not yet ready for production. These set of variables are specific for LUKS encryption configuration and installation. luks_compliant = false # Set it true if you prefer to use FIPS enable in ocp deployment luks_config = [ { thumbprint = \"\", url = \"\" }, { thumbprint = \"\", url = \"\" }, { thumbprint = \"\", url = \"\" } ] luks_filesystem_device = \"/dev/mapper/root\" #Set this value for file system device luks_format = \"xfs\" #Set value of format for filesystem luks_wife_filesystem = true #Set value of wipeFileSystem luks_device = \"/dev/disk/by-partlabel/root\" #Set value of luks device luks_label = \"luks-root\" #Set value of tang label luks_options = [\"--cipher\", \"aes-cbc-essiv:sha256\"] #Set List of luks options for the luks encryption luks_wipe_volume = true #Set value of wipeVolume luks_name = \"root\" #Set value of luks name","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#how-to-use-vartfvars","text":"How to use var.tfvars Introduction IBM Cloud Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#introduction","text":"This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf","title":"Introduction"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#ibm-cloud-details","text":"These set of variables specify the access key and PowerVS location details. ibmcloud_api_key = \"xyzaaaaaaaabcdeaaaaaa\" ibmcloud_region = \"xya\" ibmcloud_zone = \"abc\" service_instance_id = \"abc123xyzaaaa\" You'll need to create an API key to use the automation code. Please refer to the following instructions to generate API key - https://cloud.ibm.com/docs/account?topic=account-userapikey In order to retrieve the PowerVS region, zone and instance specific details please use the IBM Cloud CLI. Run ibmcloud pi service-list . It will list the service instance names with IDs. The ID will be of the form crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6:: The 6th field is the ibmcloud_zone and 8th field is the service_instance_id $ echo \"crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6::\" | cut -f6,8 -d\":\" eu-de-1:360a5df8-3f00-44b2-bd9f-d9a51fe53de6 Following are the region and zone mapping: ibmcloud_region ibmcloud_zone eu-de eu-de-1 eu-de eu-de-2 dal dal12 lon lon04 lon lon06 syd syd04 sao sao01 tor tor01 tok tok04 us-east us-east NOTE: us-east is Washington, DC datacenter. Tieing all these, the values to be used will be as shown below: ibmcloud_region = eu-de ibmcloud_zone = eu-de-1 service_instance_id = 360a5df8-3f00-44b2-bd9f-d9a51fe53de6","title":"IBM Cloud Details"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#openshift-cluster-details","text":"These set of variables specify the cluster capacity. Change the values as per your requirement. The defaults (recommended config) should suffice for most of the common use-cases. bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} You can also choose one of the default node configuration templates that are stored in the compute-vars directory, as per your requirements. The default flavors present under the compute-vars folder: small.tfvars medium.tfvars large.tfvars memory is in GBs and count specifies the number of VMs that should be created for each type. To enable high availability (HA) for cluster services running on the bastion set the bastion count value to 2. Note that in case of HA, the automation will not setup NFS storage. count of 1 for bastion implies the default non-HA bastion setup. You can optionally set the worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. To attach additional volumes to master or worker nodes, set the optional data_volume_count key to the number of volumes that is to be attached and the data_volume_size to the size (in GB) for each volume. master = {memory = \"32\", processors = \"0.5\", \"count\" = 3, data_volume_count = 0, data_volume_size = 100} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2, data_volume_count = 0, data_volume_size = 100} For PowerVS processors are equal to entitled physical count. So N processors == N physical core entitlements == ceil[N] vCPUs. Here are some examples to help you understand the relationship. Example 1 0.5 processors == 0.5 physical core entitlements == ceil[0.5] = 1 vCPU == 8 logical OS CPUs (SMT=8) Example 2 1.5 processors == 1.5 physical core entitlements == ceil[1.5] = 2 vCPU == 16 logical OS CPUs (SMT=8) Example 3 2 processors == 2 physical core entitlements == ceil[2] = 2 vCPU == 16 logical OS CPUs (SMT=8) These set of variables specify the RHEL and RHCOS boot image names. These images should have been already imported in your PowerVS service instance. Change the image names according to your environment. Ensure that you use the correct RHCOS image specific to the pre-release version rhel_image_name = \"<rhel_or_centos_image-name>\" rhcos_image_name = \"<rhcos-image-name>\" Note that the boot images should have a minimum disk size of 120GB These set of variables should be provided when RHCOS image should be imported from public bucket of cloud object storage to your PowerVS service instance rhcos_import_image = true # true/false (default=false) rhcos_import_image_filename = \"rhcos-411-85-202203181612-0-ppc64le-powervs.ova.gz\" # RHCOS boot image file name available in cloud object storage rhcos_import_image_storage_type = \"tier1\" # tier1/tier3 (default=tier1) Storage type in PowerVS where image needs to be uploaded This variable specifies the name of the private network that is configured in your PowerVS service instance. network_name = \"ocp-net\" These set of variables specify the type of processor and physical system type to be used for the VMs. Change the default values according to your requirement. processor_type = \"shared\" #Can be shared or dedicated system_type = \"s922\" #Can be either s922 or e980 These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" #Set it to an appropriate username for non-root user access public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" rhel_username is set to root. rhel_username can be set to an appropriate username having superuser privileges with no password prompt. Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details, RHEL subscription supports two methods: one is using username and password, the other is using activation key. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . If you are using CentOS as the bastion image, then leave these variables as-is. rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" Or define following variables to use activation key for RHEL subscription: rhel_subscription_org = \"org-id\" rhel_subscription_activationkey = \"activation-key\" This variable specifies the number of hardware threads (SMT) that's used for the bastion node. Default setting should be fine for majority of the use-cases. rhel_smt = 4","title":"OpenShift Cluster Details"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#openshift-installation-details","text":"These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" use_zone_info_for_names = true Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp If cluster_id_prefix is not set, the cluster_id will be used only without prefix. A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 32 characters. The use_zone_info_for_names is a flag to indicate whether to use cluster_id - ibmcloud_zone or only cluster_id as name prefix for resource naming on PowerVS. The default value is set to true to use zone info in names, and the total length of cluster_id_prefix - cluster_id - ibmcloud_zone should not exceed 32 characters.","title":"OpenShift Installation Details"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#fips-variable-for-openshift-deployment","text":"These variables will be used for deploying OCP in FIPS mode. Change the values as per your requirement. fips_compliant = false Note: Once fips_compliant set to true it will enable FIPS on the OCP cluster and also on bastion nodes. At the end of install the bastion nodes will be rebooted.","title":"FIPS Variable for OpenShift deployment"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#using-ibm-cloud-services","text":"You can use IBM Cloud classic DNS and VPC Load Balancer services for running the OCP cluster. When this feature is enabled the services called named (DNS) and haproxy (Load Balancer) will not be running on the bastion/helpernode. Ensure you have setup DirectLink with IBM Cloud VPC over the private network in cloud instance. Also, ensure you have registered a DNS domain and use it as given in cluster_domain variable. IMPORTANT : This is an experimental feature at present. Please manually set variables setup_snat = true and setup_squid_proxy = false for using IBM Cloud services. This will allow the cluster nodes have public internet access without a proxy server. Below variables needs to be set in order to use the IBM Cloud services. use_ibm_cloud_services = true ibm_cloud_vpc_name = \"ocp-vpc\" ibm_cloud_vpc_subnet_name = \"ocp-subnet\" These set of variables specify the username and API key for accessing IBM Cloud services. The default combination should suffice for most of the common use-cases. iaas_classic_username = \"apikey\" iaas_classic_api_key = \"\" # if empty, will default to ibmcloud_api_key. iaas_vpc_region = \"\" # if empty, will default to ibmcloud_region. Note: iaas_classic_username , iaas_classic_api_key and iaas_vpc_region variables are optional, These variables need to be set only when using a different classic username, key and vpc region. By default apikey will be used as the iaas_class_username , ibmcloud_api_key will be used as the iaas_classic_api_key and ibmcloud_region will be used as the iaas_vpc_region . Note that non-default values for these variables can also be passed via environment variables IAAS_CLASSIC_USERNAME and IAAS_CLASSIC_API_KEY respectively.","title":"Using IBM Cloud Services"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#misc-customizations","text":"These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/redhat-cop/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" This variable specify if bastion should poll for the Health Status to be OK or WARNING. Default is OK. bastion_health_status = \"OK\" This variable specify the MTU value for the private network interface on RHEL and RHCOS nodes. The CNI network will have - 50 for OpenshiftSDN and - 100 for OVNKubernetes network provider. private_network_mtu = 1450 These variables can be used when debugging ansible playbooks. installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable can be used to change the repository name for installing ansible package on RHEL. ansible_repo_name = \"ansible-2.9-for-rhel-8-ppc64le-rpms\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of day-1 kernel arguments for the cluster nodes. To add kernel arguments to master or worker nodes, using MachineConfig object and inject that object into the set of manifest files used by Ignition during cluster setup. rhcos_pre_kernel_options = [] Example 1 rhcos_pre_kernel_options = [\"rd.multipath=default\",\"root=/dev/disk/by-label/dm-mpath-root\"] List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] This is a Map of Node labels and its values. Some of the well known labels such as topology.kubernetes.io/region, topology.kubernetes.io/zone and node.kubernetes.io/instance-type are automated. More custom labels can be added using the node_labels map variable. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. node_labels = {} Example 1 node_labels = {\"failure-domain.beta.kubernetes.io/region\": \"mon\",\"failure-domain.beta.kubernetes.io/zone\": \"mon01\"} These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = true If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_shareable = false The following variables are specific to upgrading an existing installation. upgrade_image = \"\" #(e.g. `\"quay.io/openshift-release-dev/ocp-release-nightly@sha256:xxxxx\"`) upgrade_version = \"\" upgrade_pause_time = \"70\" upgrade_delay_time = \"600\" One of the two varaibles upgrade_image or upgrade_version is required for upgrading the cluster. upgrade_image having higher precedence than upgrade_version . The following variables are specific to performing EUS upgrades. eus_upgrade_version = \"4.11.14\" eus_upgrade_channel = \"stable-4.11\" #(stable-4.x, fast-4.x, candidate-4.x, eus-4.x) eus_upgrade_image = \"quay.io/openshift-release-dev/ocp-release:4.11.14-ppc64le\" eus_upstream = \"\" (e.g. `\"https://ppc64le.ocp.releases.ci.openshift.org/graph\"`) The following variables are specific to enable the connectivity between OCP nodes in PowerVS and IBM Cloud infrastructure over DirectLink. ibm_cloud_dl_endpoint_net_cidr = \"\" ibm_cloud_http_proxy = \"\" This variable is used to set the default Container Network Interface (CNI) network provider such as OpenShiftSDN or OVNKubernetes cni_network_provider = \"OVNKubernetes\" This variable is used to enable SNAT for OCP nodes. When using SNAT, the OCP nodes will be able to access public internet without using a proxy setup_snat = true These set of variables are specific for CSI Driver configuration and installation. csi_driver_install = false csi_driver_type = \"stable\" csi_driver_version = \"v0.1.1\" IMPORTANT : This is an experimental feature and not yet ready for production. These set of variables are specific for LUKS encryption configuration and installation. luks_compliant = false # Set it true if you prefer to use FIPS enable in ocp deployment luks_config = [ { thumbprint = \"\", url = \"\" }, { thumbprint = \"\", url = \"\" }, { thumbprint = \"\", url = \"\" } ] luks_filesystem_device = \"/dev/mapper/root\" #Set this value for file system device luks_format = \"xfs\" #Set value of format for filesystem luks_wife_filesystem = true #Set value of wipeFileSystem luks_device = \"/dev/disk/by-partlabel/root\" #Set value of luks device luks_label = \"luks-root\" #Set value of tang label luks_options = [\"--cipher\", \"aes-cbc-essiv:sha256\"] #Set List of luks options for the luks encryption luks_wipe_volume = true #Set value of wipeVolume luks_name = \"root\" #Set value of luks name","title":"Misc Customizations"},{"location":"openshift-install-power/","text":"Install OpenShift on PowerVS Install OpenShift on PowerVS Introduction Features Supported Platforms MacOS Linux (x86_64/ppc64le) Windows 10 (64-bit) Firewall Requirements Usage Prerequisites Quickstart Advanced Usage Different OpenShift Versions Non-interactive mode Tutorials Detailed Explanation of the Core Commands setup variables create destroy Contributing Introduction This project contains a bash script to help you deploy OpenShift Container Platform 4.X on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud (PowerVS). The Terraform code at ocp4-upi-powervs is used for the deployment process. Ensure your PowerVS instance is prepped for deploying OpenShift Clusters. Please check this link for more details Here is quick demo . Features Simple script based installer to deploy OpenShift (4.5 onwards) cluster on PowerVS leveraging Infrastructure as Code (IaC) pattern Supports multiple platforms including Linux(x86_64/ppc64le), Windows & Mac OSX Sets up the latest IBM Cloud CLI with Power Virtual Servers plugin. Sets up the latest Terraform binary Provides interactive mode to populate required Terraform variables Abstract out the Terraform lifecycle management Supported Platforms Only 64bit Operating Systems are supported by the script. The script requires bash shell and a package manager pre-configured. MacOS Catalina (10.15) and above is required. The script uses Homebrew package manager to install required packages. Linux (x86_64/ppc64le) RHEL8/CentOS8 and above or Ubuntu 16.04 and above is required. The script uses the default package manager ( yum/dnf/apt ) based on the distribution. Windows 10 (64-bit) The script can run on GitBash, Windows Subsystem for Linux and Cygwin terminals. If using Cygwin, then please ensure curl and unzip packages are installed. You will need to run the Cygwin setup again. Note: PowerShell is Unsupported . Firewall Requirements Ensure inbound access is allowed for the following TCP ports. This is only required when using a Cloud instance or a remote VM so that you can connect to it using SSH and run the installer 22 (SSH) Ensure outbound access is allowed for the following TCP ports 80 (HTTP) 443 (HTTPS) 6443 (OC CLI) Usage Create an install directory where all the configurations, logs and data files will be stored. $ mkdir ocp-install-dir && cd ocp-install-dir Download the script on your system and change the permission to execute. $ curl -sL https://raw.githubusercontent.com/ocp-power-automation/openshift-install-power/main/openshift-install-powervs -o ./openshift-install-powervs $ chmod +x ./openshift-install-powervs Run the script. $ ./openshift-install-powervs Automation for deploying OpenShift 4.X on PowerVS Usage: openshift-install-powervs [command] [<args> [<value>]] Available commands: setup Install all the required packages/binaries in current directory variables Interactive way to populate the variables file create Create an OpenShift cluster destroy Destroy an OpenShift cluster output Display the cluster information. Runs terraform output [NAME] access-info Display the access information of installed OpenShift cluster help Display this information Where <args>: -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) -flavor Cluster compute template to use eg: small, medium, large -force-destroy Not ask for confirmation during destroy command -ignore-os-checks Ignore operating system related checks -ignore-warnings Warning messages will not be displayed. Should be specified first, before any other args. -verbose Enable verbose for terraform console messages -all-images List all the images available during variables prompt -trace Enable tracing of all executed commands -version, -v Display the script version Environment Variables: IBMCLOUD_API_KEY IBM Cloud API key RELEASE_VER OpenShift release version (Default: 4.12) ARTIFACTS_VERSION Tag or Branch name of ocp4-upi-powervs repository (Default: main) RHEL_SUBS_PASSWORD RHEL subscription password if not provided in variables NO_OF_RETRY Number of retries/attempts to run repeatable actions such as create (Default: 5) Submit issues at: https://github.com/ocp-power-automation/openshift-install-power/issues Using the Container Image To use the images present in the quay.io/powercloud repository, run the following command. docker run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data quay.io/powercloud/openshift-install-powervs:<valid-tag> create OR podman run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data quay.io/powercloud/openshift-install-powervs:<valid-tag> create Tags provide information about the release version, ocp version or supported architecture. In case you need to build the openshift-install-powervs image, check out the following \"Usage with Containers\" link Prerequisites The script assumes PowerVS prerequisites for OpenShift are already in place. In case you missed, here is the link to the prerequisites For running the script you need the following: 1. IBM Cloud API key : Create the key by following the instructions available in the following link 2. OpenShift Pull secret : Download the secret from the following link . You'll need to place the file in the install directory and name it as pull-secret.txt 3. RHEL Subscription ID and Password . Quickstart Export the IBM Cloud API Key and RHEL Subscription Password. $ set +o history $ export IBMCLOUD_API_KEY='<your API key>' $ export RHEL_SUBS_PASSWORD='<your RHEL subscription password>' $ set -o history Run the create command. $ ./openshift-install-powervs create The script will setup the required tools and run in interactive mode prompting for inputs. Once the above command completes successfully it will print the cluster access information. ``` Login to bastion: 'ssh -i automation/data/id_rsa root@145.48.43.53' and start using the 'oc' command. To access the cluster on local system when using 'oc' run: 'export KUBECONFIG=/root/ocp-install-dir/automation/kubeconfig' Access the OpenShift web-console here: https://console-openshift-console.apps.test-ocp-6f2c.ibm.com Login to the console with user: \"kubeadmin\", and password: \"MHvmI-z5nY8-CBFKF-hmCDJ\" Add the line on local system 'hosts' file: 145.48.43.53 api.test-ocp-6f2c.ibm.com console-openshift-console.apps.test-ocp-6f2c.ibm.com integrated-oauth-server-openshift-authentication.apps.test-ocp-6f2c.ibm.com oauth-openshift.apps.test-ocp-6f2c.ibm.com prometheus-k8s-openshift-monitoring.apps.test-ocp-6f2c.ibm.com grafana-openshift-monitoring.apps.test-ocp-6f2c.ibm.com example.apps.test-ocp-6f2c.ibm.com ``` Advanced Usage Before running the script, you may choose to override some environment variables as per your requirement. Different OpenShift Versions By default OpenShift version 4.12 is installed. If you want to install 4.11, then export the variable RELEASE_VER . $ export RELEASE_VER=\"4.11\" ARTIFACTS_VERSION : Tag/Branch (eg: release-4.11, v4.11, main) of ocp4-upi-powervs repository. Default is \"main\". $ export ARTIFACTS_VERSION=\"release-4.11\" Non-interactive mode You can avoid the interactive mode by having the required input files available in the install directory Required input files 1. Terraform vars file (filename: var.tfvars ) 2. SSH key files (filename: id_rsa & id_rsa.pub ) Example `var.tfvars` file ``` ibmcloud_region = \"syd\" ibmcloud_zone = \"syd04\" service_instance_id = \"123456abc-xzz-2223434343\" rhel_image_name = \"rhel-83-12062022\" rhcos_image_name = \"rhcos-412-02012023\" network_name = \"ocp-net\" openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.12/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.12/openshift-client-linux.tar.gz\" cluster_id_prefix = \"test-ocp\" cluster_domain = \"xip.io\" storage_type = \"nfs\" volume_size = \"300\" bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} rhel_subscription_username = \"mysubscription@email.com\" rhel_subscription_password = \"mysubscriptionPassword\" ``` You can also pass a custom Terraform variables file using the option `-var-file <filename>` to the script. You can also use the option `-var \"key=value\"` to pass a single variable. If the same variable is given more than once then precedence will be from left (low) to right (high). Tutorials Check out the following learning path for deploying and using OpenShift on PowerVS Detailed Explanation of the Core Commands The following core commands are supported by the script. setup variables create destroy Below is a simple flow chart explaining the flow of each command. Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"Deploying OpenShift on IBM Cloud Power Virtual Servers via Bash Script"},{"location":"openshift-install-power/#install-openshift-on-powervs","text":"Install OpenShift on PowerVS Introduction Features Supported Platforms MacOS Linux (x86_64/ppc64le) Windows 10 (64-bit) Firewall Requirements Usage Prerequisites Quickstart Advanced Usage Different OpenShift Versions Non-interactive mode Tutorials Detailed Explanation of the Core Commands setup variables create destroy Contributing","title":"Install OpenShift on PowerVS"},{"location":"openshift-install-power/#introduction","text":"This project contains a bash script to help you deploy OpenShift Container Platform 4.X on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud (PowerVS). The Terraform code at ocp4-upi-powervs is used for the deployment process. Ensure your PowerVS instance is prepped for deploying OpenShift Clusters. Please check this link for more details Here is quick demo .","title":"Introduction"},{"location":"openshift-install-power/#features","text":"Simple script based installer to deploy OpenShift (4.5 onwards) cluster on PowerVS leveraging Infrastructure as Code (IaC) pattern Supports multiple platforms including Linux(x86_64/ppc64le), Windows & Mac OSX Sets up the latest IBM Cloud CLI with Power Virtual Servers plugin. Sets up the latest Terraform binary Provides interactive mode to populate required Terraform variables Abstract out the Terraform lifecycle management","title":"Features"},{"location":"openshift-install-power/#supported-platforms","text":"Only 64bit Operating Systems are supported by the script. The script requires bash shell and a package manager pre-configured.","title":"Supported Platforms"},{"location":"openshift-install-power/#macos","text":"Catalina (10.15) and above is required. The script uses Homebrew package manager to install required packages.","title":"MacOS"},{"location":"openshift-install-power/#linux-x86_64ppc64le","text":"RHEL8/CentOS8 and above or Ubuntu 16.04 and above is required. The script uses the default package manager ( yum/dnf/apt ) based on the distribution.","title":"Linux (x86_64/ppc64le)"},{"location":"openshift-install-power/#windows-10-64-bit","text":"The script can run on GitBash, Windows Subsystem for Linux and Cygwin terminals. If using Cygwin, then please ensure curl and unzip packages are installed. You will need to run the Cygwin setup again. Note: PowerShell is Unsupported .","title":"Windows 10 (64-bit)"},{"location":"openshift-install-power/#firewall-requirements","text":"Ensure inbound access is allowed for the following TCP ports. This is only required when using a Cloud instance or a remote VM so that you can connect to it using SSH and run the installer 22 (SSH) Ensure outbound access is allowed for the following TCP ports 80 (HTTP) 443 (HTTPS) 6443 (OC CLI)","title":"Firewall Requirements"},{"location":"openshift-install-power/#usage","text":"Create an install directory where all the configurations, logs and data files will be stored. $ mkdir ocp-install-dir && cd ocp-install-dir Download the script on your system and change the permission to execute. $ curl -sL https://raw.githubusercontent.com/ocp-power-automation/openshift-install-power/main/openshift-install-powervs -o ./openshift-install-powervs $ chmod +x ./openshift-install-powervs Run the script. $ ./openshift-install-powervs Automation for deploying OpenShift 4.X on PowerVS Usage: openshift-install-powervs [command] [<args> [<value>]] Available commands: setup Install all the required packages/binaries in current directory variables Interactive way to populate the variables file create Create an OpenShift cluster destroy Destroy an OpenShift cluster output Display the cluster information. Runs terraform output [NAME] access-info Display the access information of installed OpenShift cluster help Display this information Where <args>: -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) -flavor Cluster compute template to use eg: small, medium, large -force-destroy Not ask for confirmation during destroy command -ignore-os-checks Ignore operating system related checks -ignore-warnings Warning messages will not be displayed. Should be specified first, before any other args. -verbose Enable verbose for terraform console messages -all-images List all the images available during variables prompt -trace Enable tracing of all executed commands -version, -v Display the script version Environment Variables: IBMCLOUD_API_KEY IBM Cloud API key RELEASE_VER OpenShift release version (Default: 4.12) ARTIFACTS_VERSION Tag or Branch name of ocp4-upi-powervs repository (Default: main) RHEL_SUBS_PASSWORD RHEL subscription password if not provided in variables NO_OF_RETRY Number of retries/attempts to run repeatable actions such as create (Default: 5) Submit issues at: https://github.com/ocp-power-automation/openshift-install-power/issues Using the Container Image To use the images present in the quay.io/powercloud repository, run the following command. docker run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data quay.io/powercloud/openshift-install-powervs:<valid-tag> create OR podman run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data quay.io/powercloud/openshift-install-powervs:<valid-tag> create Tags provide information about the release version, ocp version or supported architecture. In case you need to build the openshift-install-powervs image, check out the following \"Usage with Containers\" link","title":"Usage"},{"location":"openshift-install-power/#prerequisites","text":"The script assumes PowerVS prerequisites for OpenShift are already in place. In case you missed, here is the link to the prerequisites For running the script you need the following: 1. IBM Cloud API key : Create the key by following the instructions available in the following link 2. OpenShift Pull secret : Download the secret from the following link . You'll need to place the file in the install directory and name it as pull-secret.txt 3. RHEL Subscription ID and Password .","title":"Prerequisites"},{"location":"openshift-install-power/#quickstart","text":"Export the IBM Cloud API Key and RHEL Subscription Password. $ set +o history $ export IBMCLOUD_API_KEY='<your API key>' $ export RHEL_SUBS_PASSWORD='<your RHEL subscription password>' $ set -o history Run the create command. $ ./openshift-install-powervs create The script will setup the required tools and run in interactive mode prompting for inputs. Once the above command completes successfully it will print the cluster access information. ``` Login to bastion: 'ssh -i automation/data/id_rsa root@145.48.43.53' and start using the 'oc' command. To access the cluster on local system when using 'oc' run: 'export KUBECONFIG=/root/ocp-install-dir/automation/kubeconfig' Access the OpenShift web-console here: https://console-openshift-console.apps.test-ocp-6f2c.ibm.com Login to the console with user: \"kubeadmin\", and password: \"MHvmI-z5nY8-CBFKF-hmCDJ\" Add the line on local system 'hosts' file: 145.48.43.53 api.test-ocp-6f2c.ibm.com console-openshift-console.apps.test-ocp-6f2c.ibm.com integrated-oauth-server-openshift-authentication.apps.test-ocp-6f2c.ibm.com oauth-openshift.apps.test-ocp-6f2c.ibm.com prometheus-k8s-openshift-monitoring.apps.test-ocp-6f2c.ibm.com grafana-openshift-monitoring.apps.test-ocp-6f2c.ibm.com example.apps.test-ocp-6f2c.ibm.com ```","title":"Quickstart"},{"location":"openshift-install-power/#advanced-usage","text":"Before running the script, you may choose to override some environment variables as per your requirement.","title":"Advanced Usage"},{"location":"openshift-install-power/#different-openshift-versions","text":"By default OpenShift version 4.12 is installed. If you want to install 4.11, then export the variable RELEASE_VER . $ export RELEASE_VER=\"4.11\" ARTIFACTS_VERSION : Tag/Branch (eg: release-4.11, v4.11, main) of ocp4-upi-powervs repository. Default is \"main\". $ export ARTIFACTS_VERSION=\"release-4.11\"","title":"Different OpenShift Versions"},{"location":"openshift-install-power/#non-interactive-mode","text":"You can avoid the interactive mode by having the required input files available in the install directory Required input files 1. Terraform vars file (filename: var.tfvars ) 2. SSH key files (filename: id_rsa & id_rsa.pub ) Example `var.tfvars` file ``` ibmcloud_region = \"syd\" ibmcloud_zone = \"syd04\" service_instance_id = \"123456abc-xzz-2223434343\" rhel_image_name = \"rhel-83-12062022\" rhcos_image_name = \"rhcos-412-02012023\" network_name = \"ocp-net\" openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.12/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.12/openshift-client-linux.tar.gz\" cluster_id_prefix = \"test-ocp\" cluster_domain = \"xip.io\" storage_type = \"nfs\" volume_size = \"300\" bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} rhel_subscription_username = \"mysubscription@email.com\" rhel_subscription_password = \"mysubscriptionPassword\" ``` You can also pass a custom Terraform variables file using the option `-var-file <filename>` to the script. You can also use the option `-var \"key=value\"` to pass a single variable. If the same variable is given more than once then precedence will be from left (low) to right (high).","title":"Non-interactive mode"},{"location":"openshift-install-power/#tutorials","text":"Check out the following learning path for deploying and using OpenShift on PowerVS","title":"Tutorials"},{"location":"openshift-install-power/#detailed-explanation-of-the-core-commands","text":"The following core commands are supported by the script.","title":"Detailed Explanation of the Core Commands"},{"location":"openshift-install-power/#setup","text":"","title":"setup"},{"location":"openshift-install-power/#variables","text":"","title":"variables"},{"location":"openshift-install-power/#create","text":"","title":"create"},{"location":"openshift-install-power/#destroy","text":"Below is a simple flow chart explaining the flow of each command.","title":"destroy"},{"location":"openshift-install-power/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"openshift-install-power/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Contributor Covenant Code of Conduct"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"openshift-install-power/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Attribution"},{"location":"openshift-install-power/CONTRIBUTING/","text":"Contributing This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project. Issues If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed. Pull Request Process To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/main before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Contributing"},{"location":"openshift-install-power/CONTRIBUTING/#contributing","text":"This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"openshift-install-power/CONTRIBUTING/#issues","text":"If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed.","title":"Issues"},{"location":"openshift-install-power/CONTRIBUTING/#pull-request-process","text":"To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/main before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Pull Request Process"},{"location":"openshift-install-power/docs/container/","text":"Container Images You can build and run the install script from a container. Please refer to the following instructions for building and running the container image. Change the docker commands with podman in case Podman is installed on your machine. Build using Dockerfile The Dockerfile can be used to build an image with the install script, dependencies and the required artifacts. To build from the Dockerfile please complete the following steps. Clone this repository. git clone https://github.com/ocp-power-automation/openshift-install-power.git cd openshift-install-power Run the build command. docker build -t openshift-install-powervs -f images/Dockerfile . --no-cache Use the Image To use the image to create cluster, run the following command. docker run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data openshift-install-powervs create To destroy the cluster, run the following command. docker run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data openshift-install-powervs destroy","title":"Container Images"},{"location":"openshift-install-power/docs/container/#container-images","text":"You can build and run the install script from a container. Please refer to the following instructions for building and running the container image. Change the docker commands with podman in case Podman is installed on your machine.","title":"Container Images"},{"location":"openshift-install-power/docs/container/#build-using-dockerfile","text":"The Dockerfile can be used to build an image with the install script, dependencies and the required artifacts. To build from the Dockerfile please complete the following steps. Clone this repository. git clone https://github.com/ocp-power-automation/openshift-install-power.git cd openshift-install-power Run the build command. docker build -t openshift-install-powervs -f images/Dockerfile . --no-cache","title":"Build using Dockerfile"},{"location":"openshift-install-power/docs/container/#use-the-image","text":"To use the image to create cluster, run the following command. docker run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data openshift-install-powervs create To destroy the cluster, run the following command. docker run -it -e IBMCLOUD_API_KEY=\"<key>\" -v $(pwd):/data openshift-install-powervs destroy","title":"Use the Image"},{"location":"openshift-install-power/docs/create/","text":"Command create What it does This command when executed will start off the deployment process. You can optionally use other options are listed in the help menu. Accepts arguments: -verbose Enable verbose for terraform console -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) -flavor Cluster compute template to use eg: small, medium, large The create command will check if all the tools are installed for the deployment to start. If not then it will run the setup command. Next, it will check if the variables file var.tfvars is available in the install directory or provided via argument -var-file , if not then it will run the variables command. The compute template can also be changed in the var.tfvars file using the -flavor argument via command # ./openshift-install-powervs create -flavor small . The flavors refer to the templates from https://github.com/ocp-power-automation/ocp4-upi-powervs/tree/main/compute-vars. The Terraform console log for each attempt will be stored in logs/ directory with file name as ocp4-upi-powervs_<timestamp>_apply_<attempt_number>.log . These log files can be used for debugging purpose. Usage When setup and/or variables commands are already completed. # openshift-install-powervs create [setup_tools] Verifying the latest packages and tools [powervs_login] Trying to login with the provided IBMCLOUD_API_KEY... Targeting service crn:v1:bluemix:public:power-iaas:tor01:a/65b64c1f1c29XXXXXXXXXc:4a7700b1-e318-476b-9bf6-5a88XXXXXXX981::... [init_terraform] Initializing Terraform plugins... [init_terraform] Validating Terraform code... [apply] Running terraform apply... please wait Attempt: 1/5 [retry_terraform] Completed running the terraform command. Login to bastion: 'ssh -i automation/data/id_rsa root@145.48.43.53' and start using the 'oc' command. To access the cluster on local system when using 'oc' run: 'export KUBECONFIG=/root/ocp-install-dir/automation/kubeconfig' Access the OpenShift web-console here: https://console-openshift-console.apps.test-ocp-6f2c.ibm.com Login to the console with user: \"kubeadmin\", and password: \"MHvmI-z5nY8-CBFKF-hmCDJ\" Add the line on local system 'hosts' file: 145.48.43.53 api.test-ocp-6f2c.ibm.com console-openshift-console.apps.test-ocp-6f2c.ibm.com integrated-oauth-server-openshift-authentication.apps.test-ocp-6f2c.ibm.com oauth-openshift.apps.test-ocp-6f2c.ibm.com prometheus-k8s-openshift-monitoring.apps.test-ocp-6f2c.ibm.com grafana-openshift-monitoring.apps.test-ocp-6f2c.ibm.com example.apps.test-ocp-6f2c.ibm.com [cluster_access_info] SUCCESS: Congratulations! create command completed","title":"Command create"},{"location":"openshift-install-power/docs/create/#command-create","text":"","title":"Command create"},{"location":"openshift-install-power/docs/create/#what-it-does","text":"This command when executed will start off the deployment process. You can optionally use other options are listed in the help menu. Accepts arguments: -verbose Enable verbose for terraform console -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) -flavor Cluster compute template to use eg: small, medium, large The create command will check if all the tools are installed for the deployment to start. If not then it will run the setup command. Next, it will check if the variables file var.tfvars is available in the install directory or provided via argument -var-file , if not then it will run the variables command. The compute template can also be changed in the var.tfvars file using the -flavor argument via command # ./openshift-install-powervs create -flavor small . The flavors refer to the templates from https://github.com/ocp-power-automation/ocp4-upi-powervs/tree/main/compute-vars. The Terraform console log for each attempt will be stored in logs/ directory with file name as ocp4-upi-powervs_<timestamp>_apply_<attempt_number>.log . These log files can be used for debugging purpose.","title":"What it does"},{"location":"openshift-install-power/docs/create/#usage","text":"When setup and/or variables commands are already completed. # openshift-install-powervs create [setup_tools] Verifying the latest packages and tools [powervs_login] Trying to login with the provided IBMCLOUD_API_KEY... Targeting service crn:v1:bluemix:public:power-iaas:tor01:a/65b64c1f1c29XXXXXXXXXc:4a7700b1-e318-476b-9bf6-5a88XXXXXXX981::... [init_terraform] Initializing Terraform plugins... [init_terraform] Validating Terraform code... [apply] Running terraform apply... please wait Attempt: 1/5 [retry_terraform] Completed running the terraform command. Login to bastion: 'ssh -i automation/data/id_rsa root@145.48.43.53' and start using the 'oc' command. To access the cluster on local system when using 'oc' run: 'export KUBECONFIG=/root/ocp-install-dir/automation/kubeconfig' Access the OpenShift web-console here: https://console-openshift-console.apps.test-ocp-6f2c.ibm.com Login to the console with user: \"kubeadmin\", and password: \"MHvmI-z5nY8-CBFKF-hmCDJ\" Add the line on local system 'hosts' file: 145.48.43.53 api.test-ocp-6f2c.ibm.com console-openshift-console.apps.test-ocp-6f2c.ibm.com integrated-oauth-server-openshift-authentication.apps.test-ocp-6f2c.ibm.com oauth-openshift.apps.test-ocp-6f2c.ibm.com prometheus-k8s-openshift-monitoring.apps.test-ocp-6f2c.ibm.com grafana-openshift-monitoring.apps.test-ocp-6f2c.ibm.com example.apps.test-ocp-6f2c.ibm.com [cluster_access_info] SUCCESS: Congratulations! create command completed","title":"Usage"},{"location":"openshift-install-power/docs/destroy/","text":"Command destroy What it does This command when executed will destroy the cluster that was created by the script. You can optionally use other options are listed in the help menu. Accepts arguments: -verbose Enable verbose for terraform console -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) -force-destroy Not ask for confirmation during destroy command Ensure the cluster created using the create command is always destroyed using this script itself before you delete or change the install directory or its contents. Also make use of the same variables you had used for create command. The Terraform console log for each attempt will be stored in logs/ directory with file name as ocp4-upi-powervs_<timestamp>_destroy_<attempt_number>.log . These log files can be used for debugging purpose. Usage When the response is 'no' or any other input during the confirmation. # openshift-install-powervs destroy [question] > Are you sure you want to proceed with destroy? 1) yes 2) no #? 2 - You have answered: no [destroy] ERROR: Exiting on user request When using -force-destroy option. # openshift-install-powervs destroy -force-destroy [main] WARN: Enabling forceful destruction option for terraform destroy command [destroy] Running terraform destroy... please wait Attempt: 1/5 [retry_terraform] Completed running the terraform command. [destroy] SUCCESS: Done! destroy commmand completed","title":"Command destroy"},{"location":"openshift-install-power/docs/destroy/#command-destroy","text":"","title":"Command destroy"},{"location":"openshift-install-power/docs/destroy/#what-it-does","text":"This command when executed will destroy the cluster that was created by the script. You can optionally use other options are listed in the help menu. Accepts arguments: -verbose Enable verbose for terraform console -var Terraform variable to be passed to the create/destroy command -var-file Terraform variable file name in current directory. (By default using var.tfvars) -force-destroy Not ask for confirmation during destroy command Ensure the cluster created using the create command is always destroyed using this script itself before you delete or change the install directory or its contents. Also make use of the same variables you had used for create command. The Terraform console log for each attempt will be stored in logs/ directory with file name as ocp4-upi-powervs_<timestamp>_destroy_<attempt_number>.log . These log files can be used for debugging purpose.","title":"What it does"},{"location":"openshift-install-power/docs/destroy/#usage","text":"When the response is 'no' or any other input during the confirmation. # openshift-install-powervs destroy [question] > Are you sure you want to proceed with destroy? 1) yes 2) no #? 2 - You have answered: no [destroy] ERROR: Exiting on user request When using -force-destroy option. # openshift-install-powervs destroy -force-destroy [main] WARN: Enabling forceful destruction option for terraform destroy command [destroy] Running terraform destroy... please wait Attempt: 1/5 [retry_terraform] Completed running the terraform command. [destroy] SUCCESS: Done! destroy commmand completed","title":"Usage"},{"location":"openshift-install-power/docs/flavor/","text":"Argument Flavor Following are the flavor templates that can be selected. If the flavor argument is not provided then it will use the CUSTOM configurations which are present in var.tfvars file. Flavors are listed as options when it is not used in the command as an argument. User can select either CUSTOM, large, medium or small as per the requirements. CUSTOM Flavor template This is set in var.tfvars which can be considered for custom configurations bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} Small Configuration Template bastion = {memory = \"16\", processors = \"0.5\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} Medium Configuration Template bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 3} Large Configuration Template bastion = {memory = \"64\", processors = \"1.5\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"64\", processors = \"1.5\", \"count\" = 3} worker = {memory = \"64\", processors = \"1.5\", \"count\" = 4}","title":"Argument Flavor"},{"location":"openshift-install-power/docs/flavor/#argument-flavor","text":"Following are the flavor templates that can be selected. If the flavor argument is not provided then it will use the CUSTOM configurations which are present in var.tfvars file. Flavors are listed as options when it is not used in the command as an argument. User can select either CUSTOM, large, medium or small as per the requirements.","title":"Argument Flavor"},{"location":"openshift-install-power/docs/flavor/#custom-flavor-template","text":"This is set in var.tfvars which can be considered for custom configurations bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2}","title":"CUSTOM Flavor template"},{"location":"openshift-install-power/docs/flavor/#small-configuration-template","text":"bastion = {memory = \"16\", processors = \"0.5\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2}","title":"Small Configuration Template"},{"location":"openshift-install-power/docs/flavor/#medium-configuration-template","text":"bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"32\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 3}","title":"Medium Configuration Template"},{"location":"openshift-install-power/docs/flavor/#large-configuration-template","text":"bastion = {memory = \"64\", processors = \"1.5\", \"count\" = 1} bootstrap = {memory = \"32\", processors = \"0.5\", \"count\" = 1} master = {memory = \"64\", processors = \"1.5\", \"count\" = 3} worker = {memory = \"64\", processors = \"1.5\", \"count\" = 4}","title":"Large Configuration Template"},{"location":"openshift-install-power/docs/setup/","text":"Command setup What it does This command when executed will setup the automation requirements to be run in the current directory. It will setup required system packages, IBM Cloud CLI and power-iaas plugin, Terraform and the automation code (artifacts). Install system packages Installs all the required packages/binaries in current directory. The script will install following packages using the package manager installed on your system. 1. curl 1. unzip 1. tar Note: This is not applicable for Windows OS, make sure the commands are available before running the script. Setup IBM Cloud CLI Downloads the latest IBM Cloud CLI binary. The downloaded binary is placed in the current directory. If the latest CLI is already present in the system PATH then the script will create a symbolic link in the current directory and use it. The script does not download the binary in this case. Setup IBM Cloud plug-in for Power (power-iaas) Installs the latest IBM Cloud power-iaas plug-in if the latest is not installed already. Setup Terraform Downloads the latest Terraform binary from https://releases.hashicorp.com/terraform/. The downloaded binary is placed in the current directory. If the latest Terraform is already present in the system PATH then the script will create a symbolic link in the current directory and use it. The script does not download the binary in this case. Download the artifacts Downloads the Terraform artifacts which is used to create the OpenShift 4 cluster on PowerVS at IBM Cloud. The script uses environment variable ARTIFACTS_VERSION to download the OCP on PowerVS code. ARTIFACTS_VERSION can set to the branch or tag name eg: release-4.5, release-4.6, release-4.7 (default) , v4.5.1, etc. Another environment variable you can set is RELEASE_VER to the OpenShift version you want to install. eg: 4.5, 4.6, 4.7 (default) , etc. Usage # openshift-install-powervs setup [setup] Installing dependency packages and tools [setup_tools] Verifying the latest packages and tools [setup_artifacts] Downloading code artifacts release-4.7 in ./automation Attempt: 1/5 [setup] SUCCESS: setup command completed!","title":"Command setup"},{"location":"openshift-install-power/docs/setup/#command-setup","text":"","title":"Command setup"},{"location":"openshift-install-power/docs/setup/#what-it-does","text":"This command when executed will setup the automation requirements to be run in the current directory. It will setup required system packages, IBM Cloud CLI and power-iaas plugin, Terraform and the automation code (artifacts).","title":"What it does"},{"location":"openshift-install-power/docs/setup/#install-system-packages","text":"Installs all the required packages/binaries in current directory. The script will install following packages using the package manager installed on your system. 1. curl 1. unzip 1. tar Note: This is not applicable for Windows OS, make sure the commands are available before running the script.","title":"Install system packages"},{"location":"openshift-install-power/docs/setup/#setup-ibm-cloud-cli","text":"Downloads the latest IBM Cloud CLI binary. The downloaded binary is placed in the current directory. If the latest CLI is already present in the system PATH then the script will create a symbolic link in the current directory and use it. The script does not download the binary in this case.","title":"Setup IBM Cloud CLI"},{"location":"openshift-install-power/docs/setup/#setup-ibm-cloud-plug-in-for-power-power-iaas","text":"Installs the latest IBM Cloud power-iaas plug-in if the latest is not installed already.","title":"Setup IBM Cloud plug-in for Power (power-iaas)"},{"location":"openshift-install-power/docs/setup/#setup-terraform","text":"Downloads the latest Terraform binary from https://releases.hashicorp.com/terraform/. The downloaded binary is placed in the current directory. If the latest Terraform is already present in the system PATH then the script will create a symbolic link in the current directory and use it. The script does not download the binary in this case.","title":"Setup Terraform"},{"location":"openshift-install-power/docs/setup/#download-the-artifacts","text":"Downloads the Terraform artifacts which is used to create the OpenShift 4 cluster on PowerVS at IBM Cloud. The script uses environment variable ARTIFACTS_VERSION to download the OCP on PowerVS code. ARTIFACTS_VERSION can set to the branch or tag name eg: release-4.5, release-4.6, release-4.7 (default) , v4.5.1, etc. Another environment variable you can set is RELEASE_VER to the OpenShift version you want to install. eg: 4.5, 4.6, 4.7 (default) , etc.","title":"Download the artifacts"},{"location":"openshift-install-power/docs/setup/#usage","text":"# openshift-install-powervs setup [setup] Installing dependency packages and tools [setup_tools] Verifying the latest packages and tools [setup_artifacts] Downloading code artifacts release-4.7 in ./automation Attempt: 1/5 [setup] SUCCESS: setup command completed!","title":"Usage"},{"location":"openshift-install-power/docs/variables/","text":"Command variables What it does This command when executed will run interactive prompts for gathering inputs for installing OpenShift on PowerVS. The results will be stored in a file named var.tfvars in the current directory which will be used as an input to the Terraform automation. The installation process needs pull-secret.txt in the current directory for downloading OpenShift images on the cluster. If not found, the variables command will prompt for pull-secret contents. Similar to pull-secret.txt file the script will also lookup for id_rsa & id_rsa.pub files in the current directory. If not found it will prompt to use the current login user's SSH key pair at ~/.ssh/ . If you reply a no then the script will create an SSH key pair for you in the current directory. The private key id_rsa can be used to login to the cluster. The SSH keys should be in OpenSSH format and without a passphrase. Please ensure you have exported the IBM Cloud API key using following command: export IBMCLOUD_API_KEY=<your api key> Optionally you could also export the RHEL subscription password if you do not want to store it in the variables file. export RHEL_SUBS_PASSWORD='<your subscription password>' The script will try to filter the images for selection. The argument -all-images can be used to display all the available images during the prompt for RHEL and RHCOS image. There will be series of questions mainly categorized as: Multi choice question List of options will be displayed where you need to enter the number corresponding to the choice you want to select. [question] > Select the RHEL image to use for bastion node: 1) rhel-82-10162020 2) rhel-83-11032020 #? 1 - You have answered: rhel-82-10162020 Question with default value The question will have a default value present at the end in (round-brackets). Just press enter if you want to use the default value OR type the value you want and press Enter key. [question] > Enter a short name to identify the cluster (test-ocp) ? - You have answered: test-ocp [question] > Enter a domain name for the cluster (ibm.com) ? myorg.com - You have answered: myorg.com Question in plain text The question which can be answered in plain text. Enter the value you want and press Enter key. [question] > Enter RHEL subscription username for bastion nodes ? myredhatuser - You have answered: myredhatuser Question with sensitive value The question which accept sensitive information such as passwords and pull-secret contents. [question] > Enter the password for above username. WARNING: If you do not wish to store the subscription password please export RHEL_SUBS_PASSWORD - *********** Usage # ./openshift-install-powervs variables [setup_tools] Verifying the latest packages and tools [variables] Trying to login with the provided IBMCLOUD_API_KEY... [question] > Select the Service Instance name to use: 1) ocp-cicd-toronto-01 2) ocp-internal-toronto #? 1 - You have answered: ocp-cicd-toronto-01 Targeting service crn:v1:bluemix:public:power-iaas:tor01:a/65b64c1f1c2XXXXX:4a1f10a2-0797-4ac8-9c41-XXXXXXX::... [variables] Gathering information from the selected Service Instance... Please wait [question] > Select the RHEL image to use for bastion node: 1) rhel-82-10162020 2) rhel-83-11032020 #? 1 - You have answered: rhel-82-10162020 [question] > Select the RHCOS image to use for cluster nodes: 1) rhcos-45-09242020 2) rhcos-46-09182020 3) rhcos-47-10172020 #? 2 - You have answered: rhcos-46-09182020 [question] > Select the private network to use: 1) ocp-net #? 1 - You have answered: ocp-net [question] > Select the system type to use for cluster nodes: 1) e980 2) s922 #? 2 - You have answered: s922 [question] > Select the OCP version to use: 1) 4.5.4 4) 4.5.7 7) 4.5.10 10) 4.5.13 13) 4.5.16 16) 4.5.19 19) fast-4.5 2) 4.5.5 5) 4.5.8 8) 4.5.11 11) 4.5.14 14) 4.5.17 17) 4.5.20 20) latest-4.5 3) 4.5.6 6) 4.5.9 9) 4.5.12 12) 4.5.15 15) 4.5.18 18) candidate-4.5 21) stable-4.5 #? 11 - You have answered: 4.5.14 [question] > Enter a short name to identify the cluster (test-ocp) ? - You have answered: test-ocp [question] > Enter a domain name for the cluster (ibm.com) ? myorg.com - You have answered: myorg.com [question] > Do you want to configure High Availability for bastion nodes? 1) yes 2) no #? 2 - You have answered: no [question] > Do you need NFS storage to be configured? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter the NFS volume size(GB) (300) ? - You have answered: 300 [question] > Select the flavor for the cluster nodes: 1) large 2) medium 3) small 4) CUSTOM #? 4 - You have answered: CUSTOM [question] > Do you want to use the default configuration for all the cluster nodes? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter RHEL subscription username for bastion nodes ? myredhatuser - You have answered: myredhatuser [question] > Enter the password for above username. WARNING: If you do not wish to store the subscription password please export RHEL_SUBS_PASSWORD [question] > Enter the pull-secret ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** *************************************** [question] > Found SSH key pair in /root/.ssh/ do you want to use them? (yes) ? - You have answered: yes [question] > Do you want to use IBM Cloud Classic DNS and VPC Load Balancer services? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter IBM Cloud VPC name ? ocp-vpc-fra - You have answered: ocp-vpc-fra [question] > Enter IBM Cloud VPC subnet name ? ocp-subnet - You have answered: ocp-subnet [question] > Enter IBM Cloud VPC Infrastructure region. (Power VS region will be used for VPC if you do not provide any value). ? us-south - You have answered: us-south [variables] SUCCESS: variables command completed! Using with flavor argument It skips all flavor related questions. # ./openshift-install-powervs variables -flavor small [setup_tools] Verifying the latest packages and tools [variables] Trying to login with the provided IBMCLOUD_API_KEY... [question] > Select the Service Instance name to use: 1) ocp-cicd-toronto-01 2) ocp-internal-toronto #? 1 - You have answered: ocp-cicd-toronto-01 Targeting service crn:v1:bluemix:public:power-iaas:tor01:a/65b64c1f1c2XXXXX:4a1f10a2-0797-4ac8-9c41-XXXXXXX::... [variables] Gathering information from the selected Service Instance... Please wait [question] > Select the RHEL image to use for bastion node: 1) rhel-82-10162020 2) rhel-83-11032020 #? 1 - You have answered: rhel-82-10162020 [question] > Select the RHCOS image to use for cluster nodes: 1) rhcos-45-09242020 2) rhcos-46-09182020 3) rhcos-47-10172020 #? 2 - You have answered: rhcos-46-09182020 [question] > Select the private network to use: 1) ocp-net #? 1 - You have answered: ocp-net [question] > Select the OCP version to use: 1) 4.5.4 4) 4.5.7 7) 4.5.10 10) 4.5.13 13) 4.5.16 16) 4.5.19 19) fast-4.5 2) 4.5.5 5) 4.5.8 8) 4.5.11 11) 4.5.14 14) 4.5.17 17) 4.5.20 20) latest-4.5 3) 4.5.6 6) 4.5.9 9) 4.5.12 12) 4.5.15 15) 4.5.18 18) candidate-4.5 21) stable-4.5 #? 11 - You have answered: 4.5.14 [question] > Enter a short name to identify the cluster (test-ocp) ? - You have answered: test-ocp [question] > Enter a domain name for the cluster (ibm.com) ? myorg.com - You have answered: myorg.com [question] > Do you want to configure High Availability for bastion nodes? 1) yes 2) no #? 2 - You have answered: no [question] > Do you need NFS storage to be configured? 1) yes 2) no #? 2 - You have answered: no [question] > Enter RHEL subscription username for bastion nodes ? myredhatuser - You have answered: myredhatuser [question] > Enter the password for above username. WARNING: If you do not wish to store the subscription password please export RHEL_SUBS_PASSWORD [question] > Enter the pull-secret ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** *************************************** [question] > Found SSH key pair in /root/.ssh/ do you want to use them? (yes) ? - You have answered: yes [question] > Do you want to use IBM Cloud Classic DNS and VPC Load Balancer services? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter IBM Cloud VPC name ? ocp-vpc-fra - You have answered: ocp-vpc-fra [question] > Enter IBM Cloud VPC subnet name ? ocp-subnet - You have answered: ocp-subnet [question] > Enter IBM Cloud VPC Infrastructure region. (Power VS region will be used for VPC if you do not provide any value). ? us-south - You have answered: us-south [variables] SUCCESS: variables command completed!","title":"Command variables"},{"location":"openshift-install-power/docs/variables/#command-variables","text":"","title":"Command variables"},{"location":"openshift-install-power/docs/variables/#what-it-does","text":"This command when executed will run interactive prompts for gathering inputs for installing OpenShift on PowerVS. The results will be stored in a file named var.tfvars in the current directory which will be used as an input to the Terraform automation. The installation process needs pull-secret.txt in the current directory for downloading OpenShift images on the cluster. If not found, the variables command will prompt for pull-secret contents. Similar to pull-secret.txt file the script will also lookup for id_rsa & id_rsa.pub files in the current directory. If not found it will prompt to use the current login user's SSH key pair at ~/.ssh/ . If you reply a no then the script will create an SSH key pair for you in the current directory. The private key id_rsa can be used to login to the cluster. The SSH keys should be in OpenSSH format and without a passphrase. Please ensure you have exported the IBM Cloud API key using following command: export IBMCLOUD_API_KEY=<your api key> Optionally you could also export the RHEL subscription password if you do not want to store it in the variables file. export RHEL_SUBS_PASSWORD='<your subscription password>' The script will try to filter the images for selection. The argument -all-images can be used to display all the available images during the prompt for RHEL and RHCOS image. There will be series of questions mainly categorized as:","title":"What it does"},{"location":"openshift-install-power/docs/variables/#multi-choice-question","text":"List of options will be displayed where you need to enter the number corresponding to the choice you want to select. [question] > Select the RHEL image to use for bastion node: 1) rhel-82-10162020 2) rhel-83-11032020 #? 1 - You have answered: rhel-82-10162020","title":"Multi choice question"},{"location":"openshift-install-power/docs/variables/#question-with-default-value","text":"The question will have a default value present at the end in (round-brackets). Just press enter if you want to use the default value OR type the value you want and press Enter key. [question] > Enter a short name to identify the cluster (test-ocp) ? - You have answered: test-ocp [question] > Enter a domain name for the cluster (ibm.com) ? myorg.com - You have answered: myorg.com","title":"Question with default value"},{"location":"openshift-install-power/docs/variables/#question-in-plain-text","text":"The question which can be answered in plain text. Enter the value you want and press Enter key. [question] > Enter RHEL subscription username for bastion nodes ? myredhatuser - You have answered: myredhatuser","title":"Question in plain text"},{"location":"openshift-install-power/docs/variables/#question-with-sensitive-value","text":"The question which accept sensitive information such as passwords and pull-secret contents. [question] > Enter the password for above username. WARNING: If you do not wish to store the subscription password please export RHEL_SUBS_PASSWORD - ***********","title":"Question with sensitive value"},{"location":"openshift-install-power/docs/variables/#usage","text":"# ./openshift-install-powervs variables [setup_tools] Verifying the latest packages and tools [variables] Trying to login with the provided IBMCLOUD_API_KEY... [question] > Select the Service Instance name to use: 1) ocp-cicd-toronto-01 2) ocp-internal-toronto #? 1 - You have answered: ocp-cicd-toronto-01 Targeting service crn:v1:bluemix:public:power-iaas:tor01:a/65b64c1f1c2XXXXX:4a1f10a2-0797-4ac8-9c41-XXXXXXX::... [variables] Gathering information from the selected Service Instance... Please wait [question] > Select the RHEL image to use for bastion node: 1) rhel-82-10162020 2) rhel-83-11032020 #? 1 - You have answered: rhel-82-10162020 [question] > Select the RHCOS image to use for cluster nodes: 1) rhcos-45-09242020 2) rhcos-46-09182020 3) rhcos-47-10172020 #? 2 - You have answered: rhcos-46-09182020 [question] > Select the private network to use: 1) ocp-net #? 1 - You have answered: ocp-net [question] > Select the system type to use for cluster nodes: 1) e980 2) s922 #? 2 - You have answered: s922 [question] > Select the OCP version to use: 1) 4.5.4 4) 4.5.7 7) 4.5.10 10) 4.5.13 13) 4.5.16 16) 4.5.19 19) fast-4.5 2) 4.5.5 5) 4.5.8 8) 4.5.11 11) 4.5.14 14) 4.5.17 17) 4.5.20 20) latest-4.5 3) 4.5.6 6) 4.5.9 9) 4.5.12 12) 4.5.15 15) 4.5.18 18) candidate-4.5 21) stable-4.5 #? 11 - You have answered: 4.5.14 [question] > Enter a short name to identify the cluster (test-ocp) ? - You have answered: test-ocp [question] > Enter a domain name for the cluster (ibm.com) ? myorg.com - You have answered: myorg.com [question] > Do you want to configure High Availability for bastion nodes? 1) yes 2) no #? 2 - You have answered: no [question] > Do you need NFS storage to be configured? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter the NFS volume size(GB) (300) ? - You have answered: 300 [question] > Select the flavor for the cluster nodes: 1) large 2) medium 3) small 4) CUSTOM #? 4 - You have answered: CUSTOM [question] > Do you want to use the default configuration for all the cluster nodes? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter RHEL subscription username for bastion nodes ? myredhatuser - You have answered: myredhatuser [question] > Enter the password for above username. WARNING: If you do not wish to store the subscription password please export RHEL_SUBS_PASSWORD [question] > Enter the pull-secret ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** *************************************** [question] > Found SSH key pair in /root/.ssh/ do you want to use them? (yes) ? - You have answered: yes [question] > Do you want to use IBM Cloud Classic DNS and VPC Load Balancer services? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter IBM Cloud VPC name ? ocp-vpc-fra - You have answered: ocp-vpc-fra [question] > Enter IBM Cloud VPC subnet name ? ocp-subnet - You have answered: ocp-subnet [question] > Enter IBM Cloud VPC Infrastructure region. (Power VS region will be used for VPC if you do not provide any value). ? us-south - You have answered: us-south [variables] SUCCESS: variables command completed!","title":"Usage"},{"location":"openshift-install-power/docs/variables/#using-with-flavor-argument","text":"It skips all flavor related questions. # ./openshift-install-powervs variables -flavor small [setup_tools] Verifying the latest packages and tools [variables] Trying to login with the provided IBMCLOUD_API_KEY... [question] > Select the Service Instance name to use: 1) ocp-cicd-toronto-01 2) ocp-internal-toronto #? 1 - You have answered: ocp-cicd-toronto-01 Targeting service crn:v1:bluemix:public:power-iaas:tor01:a/65b64c1f1c2XXXXX:4a1f10a2-0797-4ac8-9c41-XXXXXXX::... [variables] Gathering information from the selected Service Instance... Please wait [question] > Select the RHEL image to use for bastion node: 1) rhel-82-10162020 2) rhel-83-11032020 #? 1 - You have answered: rhel-82-10162020 [question] > Select the RHCOS image to use for cluster nodes: 1) rhcos-45-09242020 2) rhcos-46-09182020 3) rhcos-47-10172020 #? 2 - You have answered: rhcos-46-09182020 [question] > Select the private network to use: 1) ocp-net #? 1 - You have answered: ocp-net [question] > Select the OCP version to use: 1) 4.5.4 4) 4.5.7 7) 4.5.10 10) 4.5.13 13) 4.5.16 16) 4.5.19 19) fast-4.5 2) 4.5.5 5) 4.5.8 8) 4.5.11 11) 4.5.14 14) 4.5.17 17) 4.5.20 20) latest-4.5 3) 4.5.6 6) 4.5.9 9) 4.5.12 12) 4.5.15 15) 4.5.18 18) candidate-4.5 21) stable-4.5 #? 11 - You have answered: 4.5.14 [question] > Enter a short name to identify the cluster (test-ocp) ? - You have answered: test-ocp [question] > Enter a domain name for the cluster (ibm.com) ? myorg.com - You have answered: myorg.com [question] > Do you want to configure High Availability for bastion nodes? 1) yes 2) no #? 2 - You have answered: no [question] > Do you need NFS storage to be configured? 1) yes 2) no #? 2 - You have answered: no [question] > Enter RHEL subscription username for bastion nodes ? myredhatuser - You have answered: myredhatuser [question] > Enter the password for above username. WARNING: If you do not wish to store the subscription password please export RHEL_SUBS_PASSWORD [question] > Enter the pull-secret ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** ******************************************************************************************************************************** *************************************** [question] > Found SSH key pair in /root/.ssh/ do you want to use them? (yes) ? - You have answered: yes [question] > Do you want to use IBM Cloud Classic DNS and VPC Load Balancer services? 1) yes 2) no #? 1 - You have answered: yes [question] > Enter IBM Cloud VPC name ? ocp-vpc-fra - You have answered: ocp-vpc-fra [question] > Enter IBM Cloud VPC subnet name ? ocp-subnet - You have answered: ocp-subnet [question] > Enter IBM Cloud VPC Infrastructure region. (Power VS region will be used for VPC if you do not provide any value). ? us-south - You have answered: us-south [variables] SUCCESS: variables command completed!","title":"Using with flavor argument"},{"location":"openshift-install-power/release/","text":"Create release Ensure you have the hub client installed Update VERSION in devel Update VERSION file Update VERSION string in openshift-install-powervs script Create PR and get it merged Push changes from devel to master git clone https://github.com/ocp-power-automation/openshift-install-power git checkout master git merge origin/devel Verify all changes from devel are in master branch and then push the changes git push origin master:master Create Release cd release ./make_release -p tag This will create a tag based on latest version in VERSION and make a release. It'll also print the instructions to be followed to update the release notes.","title":"Create release"},{"location":"openshift-install-power/release/#create-release","text":"Ensure you have the hub client installed","title":"Create release"},{"location":"openshift-install-power/release/#update-version-in-devel","text":"Update VERSION file Update VERSION string in openshift-install-powervs script Create PR and get it merged","title":"Update VERSION in devel"},{"location":"openshift-install-power/release/#push-changes-from-devel-to-master","text":"git clone https://github.com/ocp-power-automation/openshift-install-power git checkout master git merge origin/devel Verify all changes from devel are in master branch and then push the changes git push origin master:master","title":"Push changes from devel to master"},{"location":"openshift-install-power/release/#create-release_1","text":"cd release ./make_release -p tag This will create a tag based on latest version in VERSION and make a release. It'll also print the instructions to be followed to update the release notes.","title":"Create Release"}]}